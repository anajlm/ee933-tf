{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "613098c2-149c-4895-ae41-74d2c3ca4360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Diagnosis  Radius_mean  Texture_mean  Perimeter_mean  Area_mean  \\\n",
      "0         M        17.99         10.38          122.80     1001.0   \n",
      "1         M        20.57         17.77          132.90     1326.0   \n",
      "2         M        19.69         21.25          130.00     1203.0   \n",
      "3         M        11.42         20.38           77.58      386.1   \n",
      "4         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   Smoothness_mean  Compactness_mean  Concavity_mean  Concave_points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   Symmetry_mean  ...  Radius_worst  Texture_worst  Perimeter_worst  \\\n",
      "0         0.2419  ...         25.38          17.33           184.60   \n",
      "1         0.1812  ...         24.99          23.41           158.80   \n",
      "2         0.2069  ...         23.57          25.53           152.50   \n",
      "3         0.2597  ...         14.91          26.50            98.87   \n",
      "4         0.1809  ...         22.54          16.67           152.20   \n",
      "\n",
      "   Area_worst  Smoothness_worst  Compactness_worst  Concavity_worst  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   Concave_points_worst  Symmetry_worst  Fractal_dimension_worst  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv \n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "percentualTreinamento = 0.7\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "# Define the numeric labels to filter\n",
    "labels={'M','B'}\n",
    "labels_list = list(labels)\n",
    "\n",
    "\n",
    "# Define column names based on dataset documentation\n",
    "columns = [\n",
    "    \"ID\", \"Diagnosis\",\n",
    "    \"Radius_mean\", \"Texture_mean\", \"Perimeter_mean\", \"Area_mean\", \"Smoothness_mean\",\n",
    "    \"Compactness_mean\", \"Concavity_mean\", \"Concave_points_mean\", \"Symmetry_mean\", \"Fractal_dimension_mean\",\n",
    "    \"Radius_se\", \"Texture_se\", \"Perimeter_se\", \"Area_se\", \"Smoothness_se\",\n",
    "    \"Compactness_se\", \"Concavity_se\", \"Concave_points_se\", \"Symmetry_se\", \"Fractal_dimension_se\",\n",
    "    \"Radius_worst\", \"Texture_worst\", \"Perimeter_worst\", \"Area_worst\", \"Smoothness_worst\",\n",
    "    \"Compactness_worst\", \"Concavity_worst\", \"Concave_points_worst\", \"Symmetry_worst\", \"Fractal_dimension_worst\"\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "dadosBrutos = pd.read_csv(\"../dataset/wdbc.data\", names=columns, header=None)\n",
    "\n",
    "# Drop the ID column (not useful for analysis)\n",
    "dadosBrutos.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "\n",
    "# Filter rows where a column equals one of the labels\n",
    "dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "# metadata \n",
    "print(dadosBrutos.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ebd0f445-ffdc-4e84-9290-1a2cc72331eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nf/hkgtx45s3f7_s6zv1l__8jw40000gp/T/ipykernel_45647/392804951.py:38: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  trainLabels = trainLabels.replace({labels_list[0]: 0, labels_list[1]: 1}).to_numpy()\n",
      "/var/folders/nf/hkgtx45s3f7_s6zv1l__8jw40000gp/T/ipykernel_45647/392804951.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  testLabels = testLabels.replace({labels_list[0]: 0, labels_list[1]: 1}) .to_numpy()\n"
     ]
    }
   ],
   "source": [
    "# Total number of samples\n",
    "\n",
    "nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 1 - treinamento\n",
    "indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "# Select remaining indices for group 1 - teste\n",
    "indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "# Determine the size of group 2\n",
    "nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 2 - treinamento\n",
    "indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "\n",
    "trainData = dadosBrutosLabel0.loc[indices_label0_treinamento]\n",
    "buffer_trainData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "testData = dadosBrutosLabel0.loc[indices_label0_teste]\n",
    "buffer_testData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "trainData = filtered_rows_Train.iloc[:, 1:31].to_numpy()\n",
    "trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "testData = filtered_rows_Test.iloc[:, 1:31].to_numpy()\n",
    "testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "# aleatoriza ordem de treinamento\n",
    "#indicesShuffledTranData = np.random.choice(trainData.index, size=1, replace=False)\n",
    "#trainData = trainData.loc[indicesShuffledTranData]\n",
    "#trainLabels = trainLabels.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "trainLabels = trainLabels.replace({labels_list[0]: 0, labels_list[1]: 1}).to_numpy()\n",
    "testLabels = testLabels.replace({labels_list[0]: 0, labels_list[1]: 1}) .to_numpy()\n",
    "\n",
    "print(trainLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac1f8a57-717a-45cd-847c-a057d9a16a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observados_treinamento= [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
      "soma= 397\n",
      "len= 397\n",
      "1.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "\n",
    "xgb_classifier = xgb.XGBClassifier(n_estimators=100, objective='binary:logistic', tree_method='hist', eta=0.1, max_depth=3, enable_categorical=True)\n",
    "xgb_classifier.fit(trainData, trainLabels)\n",
    "#clf.fit(trainData,trainLabels)\n",
    "\n",
    "observados_treinamento=xgb_classifier.predict(trainData)\n",
    "print(\"observados_treinamento=\",observados_treinamento)\n",
    "print(\"soma=\",sum(trainLabels==observados_treinamento))\n",
    "print(\"len=\",len(trainLabels))\n",
    "\n",
    "print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "\n",
    "\n",
    "MSE_treinamento = 1-(1/observados_treinamento.shape[0])*(np.power(np.sum(observados_treinamento-trainLabels),2))\n",
    "print(MSE_treinamento*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cba18-cc5b-4843-92d1-76bffdb47a46",
   "metadata": {},
   "source": [
    "## Loop Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e4de44d6-01fc-469d-96f0-99343c4d1925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocos de erro entrada:  [0.    0.006 0.012 0.018 0.024 0.03  0.036 0.042 0.048]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:20,  2.53s/it]"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "percentualErrosMaximo=0.05\n",
    "stepErros=0.006\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "print(\"Blocos de erro entrada: \", vector)\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators = 15), # n_estimators =100\n",
    "    \"SVM (RBF)\": SVC(C=100,kernel='rbf', gamma=0.001,class_weight='balanced'),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators = 20, objective = 'binary:logistic', tree_method = 'hist', eta = 0.1, # n_estimators = 100\n",
    "                             max_depth = 3, enable_categorical = True),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='sgd',\n",
    "                    max_iter=500, alpha=0.001)\n",
    "}\n",
    "# MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='lbfgs',\n",
    "                    #max_iter=3000, alpha=0.001)\n",
    "\n",
    "nRepeticoes=10\n",
    "resultadosConsolidados_treino = []\n",
    "resultadosConsolidados_teste = []\n",
    "blocos = []\n",
    "all_results=[]\n",
    "resultadosAcc=[]\n",
    "\n",
    "\n",
    "for nivelRuidoAtual in tqdm(vector):\n",
    "    for indiceRepeticao in range(nRepeticoes):\n",
    "        # Total number of samples\n",
    "\n",
    "        dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "        dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "        nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "        tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento0 = int(nAmostras_treinamento0 * (nivelRuidoAtual/2) )\n",
    "\n",
    "        # Randomly select indices for group 1 - treinamento\n",
    "        indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "        indices_label0_erro =np.random.choice(indices_label0_treinamento, size=tamanhoErroTreinamento0, replace=False)\n",
    "\n",
    "        # Select remaining indices for group 1 - teste\n",
    "        indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "        # Determine the size of group 2\n",
    "        nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "        tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento1 = int(nAmostras_treinamento0 * (nivelRuidoAtual/2) )\n",
    "\n",
    "        # Randomly select indices for group 2 - treinamento\n",
    "        indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "        indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "        indices_label1_erro =np.random.choice(indices_label1_treinamento, size=tamanhoErroTreinamento1, replace=False)\n",
    "\n",
    "        # introduz erro\n",
    "        dadosLabel0=dadosBrutosLabel0\n",
    "        dadosLabel1=dadosBrutosLabel1\n",
    "        dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"] = dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "        dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"] = dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "\n",
    "        trainData = dadosLabel0.loc[indices_label0_treinamento]\n",
    "        buffer_trainData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "        testData = dadosLabel0.loc[indices_label0_teste]\n",
    "        buffer_testData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "        filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "        filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "        #aleatoriza ordem de treinamento\n",
    "        indicesShuffledTranData = np.random.choice(filtered_rows_Train.index, size=1, replace=False)\n",
    "        trainData = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        trainLabels = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "        trainData = filtered_rows_Train.iloc[:, 1:31]\n",
    "        trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "        testData = filtered_rows_Test.iloc[:, 1:31]\n",
    "        testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "\n",
    "        trainLabels = filtered_rows_Train.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "        testLabels = filtered_rows_Test.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "          \n",
    "        # Calcula o nivel de ruido real baseado no número de indices flipados divididio pelo numero total de indices \n",
    "        nivelRuidoAtual_real=(len(indices_label0_erro)+len(indices_label1_erro))/len(trainLabels)\n",
    "        \n",
    "        ### Treinamentos\n",
    "        for name, model in classifiers.items():\n",
    "            model.fit(trainData, trainLabels)\n",
    "            resultadoTreinamento=model.predict(trainData)\n",
    "            resultadoTeste=model.predict(testData)\n",
    "            acc_teste = ( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "            resultadosAcc.append([nivelRuidoAtual_real, name, indiceRepeticao,acc_teste])\n",
    "            for true_label, pred_label in zip(resultadoTeste, testLabels):\n",
    "                all_results.append([nivelRuidoAtual_real, name, indiceRepeticao, true_label, pred_label, acc_teste])\n",
    "\n",
    "        \n",
    "      \n",
    "    # print(\"observados_treinamento=\",observados_treinamento)\n",
    "    # print(\"soma=\",sum(trainLabels==observados_treinamento))æ\n",
    "    # print(\"len=\",len(trainLabels))\n",
    "\n",
    "    # print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "print(blocos)\n",
    "\n",
    "df_results = pd.DataFrame(all_results, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"True Label\", \"Predicted Label\", \"Accuracy\"])\n",
    "resultadosExp = pd.DataFrame(resultadosAcc, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0fae4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Noise Level     Classifier  IndexRep  Accuracy\n",
      "0       0.000000  Random Forest         0  0.987220\n",
      "1       0.000000      SVM (RBF)         0  0.977636\n",
      "2       0.000000        XGBoost         0  0.987220\n",
      "3       0.000000            MLP         0  0.929712\n",
      "4       0.000000  Random Forest         1  0.977636\n",
      "..           ...            ...       ...       ...\n",
      "355     0.025189            MLP         8  0.904153\n",
      "356     0.025189  Random Forest         9  0.987220\n",
      "357     0.025189      SVM (RBF)         9  0.968051\n",
      "358     0.025189        XGBoost         9  0.984026\n",
      "359     0.025189            MLP         9  0.894569\n",
      "\n",
      "[360 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(resultadosExp)\n",
    "string = \"../resultados/estudo_piloto.csv\" \n",
    "if not os.path.exists(string):\n",
    "    resultadosExp.to_csv(string, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
