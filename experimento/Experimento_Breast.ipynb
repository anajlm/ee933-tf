{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "613098c2-149c-4895-ae41-74d2c3ca4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv \n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "percentualTreinamento = 0.7\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "# Define the numeric labels to filter\n",
    "labels={'M','B'}\n",
    "labels_list = list(labels)\n",
    "\n",
    "\n",
    "# Define column names based on dataset documentation\n",
    "columns = [\n",
    "    \"ID\", \"Diagnosis\",\n",
    "    \"Radius_mean\", \"Texture_mean\", \"Perimeter_mean\", \"Area_mean\", \"Smoothness_mean\",\n",
    "    \"Compactness_mean\", \"Concavity_mean\", \"Concave_points_mean\", \"Symmetry_mean\", \"Fractal_dimension_mean\",\n",
    "    \"Radius_se\", \"Texture_se\", \"Perimeter_se\", \"Area_se\", \"Smoothness_se\",\n",
    "    \"Compactness_se\", \"Concavity_se\", \"Concave_points_se\", \"Symmetry_se\", \"Fractal_dimension_se\",\n",
    "    \"Radius_worst\", \"Texture_worst\", \"Perimeter_worst\", \"Area_worst\", \"Smoothness_worst\",\n",
    "    \"Compactness_worst\", \"Concavity_worst\", \"Concave_points_worst\", \"Symmetry_worst\", \"Fractal_dimension_worst\"\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "dadosBrutos = pd.read_csv(\"../dataset/wdbc.data\", names=columns, header=None)\n",
    "\n",
    "# Drop the ID column (not useful for analysis)\n",
    "dadosBrutos.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "\n",
    "# Filter rows where a column equals one of the labels\n",
    "dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "# metadata \n",
    "#print(dadosBrutos.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebd0f445-ffdc-4e84-9290-1a2cc72331eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nf/hkgtx45s3f7_s6zv1l__8jw40000gp/T/ipykernel_62097/392804951.py:38: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  trainLabels = trainLabels.replace({labels_list[0]: 0, labels_list[1]: 1}).to_numpy()\n",
      "/var/folders/nf/hkgtx45s3f7_s6zv1l__8jw40000gp/T/ipykernel_62097/392804951.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  testLabels = testLabels.replace({labels_list[0]: 0, labels_list[1]: 1}) .to_numpy()\n"
     ]
    }
   ],
   "source": [
    "# Total number of samples\n",
    "\n",
    "nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 1 - treinamento\n",
    "indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "# Select remaining indices for group 1 - teste\n",
    "indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "# Determine the size of group 2\n",
    "nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 2 - treinamento\n",
    "indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "\n",
    "trainData = dadosBrutosLabel0.loc[indices_label0_treinamento]\n",
    "buffer_trainData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "testData = dadosBrutosLabel0.loc[indices_label0_teste]\n",
    "buffer_testData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "trainData = filtered_rows_Train.iloc[:, 1:31].to_numpy()\n",
    "trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "testData = filtered_rows_Test.iloc[:, 1:31].to_numpy()\n",
    "testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "# aleatoriza ordem de treinamento\n",
    "#indicesShuffledTranData = np.random.choice(trainData.index, size=1, replace=False)\n",
    "#trainData = trainData.loc[indicesShuffledTranData]\n",
    "#trainLabels = trainLabels.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "trainLabels = trainLabels.replace({labels_list[0]: 0, labels_list[1]: 1}).to_numpy()\n",
    "testLabels = testLabels.replace({labels_list[0]: 0, labels_list[1]: 1}) .to_numpy()\n",
    "\n",
    "print(trainLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1c437033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocos de erro entrada:  [0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13\n",
      " 0.14 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27] Tamanho:  28\n"
     ]
    }
   ],
   "source": [
    "percentualErrosMaximo=0.275\n",
    "stepErros=0.01\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "print(\"Blocos de erro entrada: \", vector, \"Tamanho: \", len(vector))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cba18-cc5b-4843-92d1-76bffdb47a46",
   "metadata": {},
   "source": [
    "## Experimento 1: Ruído distribuído de forma igual aos dois rótulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4de44d6-01fc-469d-96f0-99343c4d1925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocos de erro entrada:  [0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13\n",
      " 0.14 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27] Tamanho:  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:13<00:00,  6.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "percentualErrosMaximo=0.275\n",
    "stepErros=0.01\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "print(\"Blocos de erro entrada: \", vector, \"Tamanho: \", len(vector))\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators = 15,random_state = SEED), # n_estimators =100\n",
    "    \"SVM (RBF)\": SVC(C=100,kernel='rbf', gamma=0.001,class_weight='balanced',random_state = SEED),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators = 20, objective = 'binary:logistic', tree_method = 'hist', eta = 0.1, # n_estimators = 100\n",
    "                             max_depth = 3, enable_categorical = True,random_state = SEED),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='sgd',\n",
    "                    max_iter=500, alpha=0.001,random_state = SEED)\n",
    "}\n",
    "# MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='lbfgs',\n",
    "                    #max_iter=3000, alpha=0.001)\n",
    "\n",
    "nRepeticoes=18\n",
    "resultadosConsolidados_treino = []\n",
    "resultadosConsolidados_teste = []\n",
    "blocos = []\n",
    "all_results=[]\n",
    "resultadosAcc=[]\n",
    "\n",
    "\n",
    "for nivelRuidoAtual in tqdm(vector):\n",
    "    for indiceRepeticao in range(nRepeticoes):\n",
    "        # Total number of samples\n",
    "\n",
    "        dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "        dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "        nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "        tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento0 = int(nAmostras_treinamento0 * (nivelRuidoAtual/2) )\n",
    "\n",
    "        # Randomly select indices for group 1 - treinamento\n",
    "        indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "        indices_label0_erro =np.random.choice(indices_label0_treinamento, size=tamanhoErroTreinamento0, replace=False)\n",
    "\n",
    "        # Select remaining indices for group 1 - teste\n",
    "        indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "        # Determine the size of group 2\n",
    "        nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "        tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento1 = int(nAmostras_treinamento0 * (nivelRuidoAtual/2) )\n",
    "\n",
    "        # Randomly select indices for group 2 - treinamento\n",
    "        indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "        indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "        indices_label1_erro =np.random.choice(indices_label1_treinamento, size=tamanhoErroTreinamento1, replace=False)\n",
    "\n",
    "        # introduz erro\n",
    "        dadosLabel0=dadosBrutosLabel0\n",
    "        dadosLabel1=dadosBrutosLabel1\n",
    "        dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"] = dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "        dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"] = dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "\n",
    "        trainData = dadosLabel0.loc[indices_label0_treinamento]\n",
    "        buffer_trainData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "        testData = dadosLabel0.loc[indices_label0_teste]\n",
    "        buffer_testData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "        filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "        filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "        #aleatoriza ordem de treinamento\n",
    "        indicesShuffledTranData = np.random.choice(filtered_rows_Train.index, size=1, replace=False)\n",
    "        trainData = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        trainLabels = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "        trainData = filtered_rows_Train.iloc[:, 1:31]\n",
    "        trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "        testData = filtered_rows_Test.iloc[:, 1:31]\n",
    "        testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "\n",
    "        trainLabels = filtered_rows_Train.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "        testLabels = filtered_rows_Test.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "          \n",
    "        # Calcula o nivel de ruido real baseado no número de indices flipados divididio pelo numero total de indices \n",
    "        nivelRuidoAtual_real=(len(indices_label0_erro)+len(indices_label1_erro))/len(trainLabels)\n",
    "        \n",
    "        ### Treinamentos\n",
    "        for name, model in classifiers.items():\n",
    "            model.fit(trainData, trainLabels)\n",
    "            resultadoTreinamento=model.predict(trainData)\n",
    "            resultadoTeste=model.predict(testData)\n",
    "            acc_teste = ( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "            resultadosAcc.append([nivelRuidoAtual_real, name, indiceRepeticao,acc_teste])\n",
    "            for true_label, pred_label in zip(resultadoTeste, testLabels):\n",
    "                all_results.append([nivelRuidoAtual_real, name, indiceRepeticao, true_label, pred_label, acc_teste])\n",
    "\n",
    "        \n",
    "      \n",
    "    # print(\"observados_treinamento=\",observados_treinamento)\n",
    "    # print(\"soma=\",sum(trainLabels==observados_treinamento))æ\n",
    "    # print(\"len=\",len(trainLabels))\n",
    "\n",
    "    # print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "print(blocos)\n",
    "\n",
    "df_results = pd.DataFrame(all_results, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"True Label\", \"Predicted Label\", \"Accuracy\"])\n",
    "resultadosExp = pd.DataFrame(resultadosAcc, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0fae4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Noise Level     Classifier  IndexRep  Accuracy\n",
      "0        0.000000  Random Forest         0  0.984026\n",
      "1        0.000000      SVM (RBF)         0  0.977636\n",
      "2        0.000000        XGBoost         0  0.984026\n",
      "3        0.000000            MLP         0  0.904153\n",
      "4        0.000000  Random Forest         1  0.980831\n",
      "...           ...            ...       ...       ...\n",
      "2011     0.141058            MLP        16  0.859425\n",
      "2012     0.141058  Random Forest        17  0.964856\n",
      "2013     0.141058      SVM (RBF)        17  0.948882\n",
      "2014     0.141058        XGBoost        17  0.884984\n",
      "2015     0.141058            MLP        17  0.814696\n",
      "\n",
      "[2016 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(resultadosExp)\n",
    "string = \"../resultados/experimento/resultadosExperimento1.csv\" \n",
    "if not os.path.exists(string):\n",
    "    resultadosExp.to_csv(string, index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1510a78d",
   "metadata": {},
   "source": [
    "## Experimento 2: Médico tendencioso a classificar como tumor maligno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ab336fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocos de erro entrada:  [0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13\n",
      " 0.14 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27] Tamanho:  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:14<00:00,  6.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "percentualErrosMaximo=0.275\n",
    "stepErros=0.01\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "print(\"Blocos de erro entrada: \", vector, \"Tamanho: \", len(vector))\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators = 15,random_state = SEED), # n_estimators =100\n",
    "    \"SVM (RBF)\": SVC(C=100,kernel='rbf', gamma=0.001,class_weight='balanced',random_state = SEED),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators = 20, objective = 'binary:logistic', tree_method = 'hist', eta = 0.1, # n_estimators = 100\n",
    "                             max_depth = 3, enable_categorical = True,random_state = SEED),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='sgd',\n",
    "                    max_iter=500, alpha=0.001,random_state = SEED)\n",
    "}\n",
    "# MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='lbfgs',\n",
    "                    #max_iter=3000, alpha=0.001)\n",
    "\n",
    "nRepeticoes=18\n",
    "resultadosConsolidados_treino = []\n",
    "resultadosConsolidados_teste = []\n",
    "blocos = []\n",
    "all_results=[]\n",
    "resultadosAcc=[]\n",
    "\n",
    "\n",
    "for nivelRuidoAtual in tqdm(vector):\n",
    "    for indiceRepeticao in range(nRepeticoes):\n",
    "        # Total number of samples\n",
    "\n",
    "        dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] # maligno\n",
    "        dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "        nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "        tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento0 = 0 # tendencioso a classificar maligno\n",
    "\n",
    "        # Randomly select indices for group 1 - treinamento\n",
    "        indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "        indices_label0_erro =np.random.choice(indices_label0_treinamento, size=tamanhoErroTreinamento0, replace=False)\n",
    "\n",
    "        # Select remaining indices for group 1 - teste\n",
    "        indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "        # Determine the size of group 2\n",
    "        nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "        tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento1 = int(nAmostras_treinamento0 * (nivelRuidoAtual) ) # tendencioso a classificar maligno\n",
    "\n",
    "        # Randomly select indices for group 2 - treinamento\n",
    "        indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "        indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "        indices_label1_erro =np.random.choice(indices_label1_treinamento, size=tamanhoErroTreinamento1, replace=False)\n",
    "\n",
    "        # introduz erro\n",
    "        dadosLabel0=dadosBrutosLabel0\n",
    "        dadosLabel1=dadosBrutosLabel1\n",
    "        dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"] = dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "        dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"] = dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "\n",
    "        trainData = dadosLabel0.loc[indices_label0_treinamento]\n",
    "        buffer_trainData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "        testData = dadosLabel0.loc[indices_label0_teste]\n",
    "        buffer_testData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "        filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "        filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "        #aleatoriza ordem de treinamento\n",
    "        indicesShuffledTranData = np.random.choice(filtered_rows_Train.index, size=1, replace=False)\n",
    "        trainData = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        trainLabels = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "        trainData = filtered_rows_Train.iloc[:, 1:31]\n",
    "        trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "        testData = filtered_rows_Test.iloc[:, 1:31]\n",
    "        testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "\n",
    "        trainLabels = filtered_rows_Train.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "        testLabels = filtered_rows_Test.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "\n",
    "\n",
    "        # Calcula o nivel de ruido real baseado no número de indices flipados divididio pelo numero total de indices \n",
    "        nivelRuidoAtual_real=(len(indices_label0_erro)+len(indices_label1_erro))/len(trainLabels)\n",
    "        \n",
    "        ### Treinamentos\n",
    "        for name, model in classifiers.items():\n",
    "            model.fit(trainData, trainLabels)\n",
    "            resultadoTreinamento=model.predict(trainData)\n",
    "            resultadoTeste=model.predict(testData)\n",
    "            acc_teste = ( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "            resultadosAcc.append([nivelRuidoAtual_real, name, indiceRepeticao,acc_teste])\n",
    "            for true_label, pred_label in zip(resultadoTeste, testLabels):\n",
    "                all_results.append([nivelRuidoAtual_real, name, indiceRepeticao, true_label, pred_label, acc_teste])\n",
    "\n",
    "        \n",
    "      \n",
    "    # print(\"observados_treinamento=\",observados_treinamento)\n",
    "    # print(\"soma=\",sum(trainLabels==observados_treinamento))æ\n",
    "    # print(\"len=\",len(trainLabels))\n",
    "\n",
    "    # print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "print(blocos)\n",
    "\n",
    "df_results2 = pd.DataFrame(all_results, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"True Label\", \"Predicted Label\", \"Accuracy\"])\n",
    "resultadosExp2 = pd.DataFrame(resultadosAcc, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2d2304d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Noise Level     Classifier  IndexRep  Accuracy\n",
      "0        0.000000  Random Forest         0  0.984026\n",
      "1        0.000000      SVM (RBF)         0  0.977636\n",
      "2        0.000000        XGBoost         0  0.984026\n",
      "3        0.000000            MLP         0  0.904153\n",
      "4        0.000000  Random Forest         1  0.980831\n",
      "...           ...            ...       ...       ...\n",
      "2011     0.143577            MLP        16  0.769968\n",
      "2012     0.143577  Random Forest        17  0.964856\n",
      "2013     0.143577      SVM (RBF)        17  0.971246\n",
      "2014     0.143577        XGBoost        17  0.840256\n",
      "2015     0.143577            MLP        17  0.731629\n",
      "\n",
      "[2016 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(resultadosExp2)\n",
    "string = \"../resultados/experimento/resultadosExperimento2.csv\" \n",
    "if not os.path.exists(string):\n",
    "    resultadosExp2.to_csv(string, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b622a7",
   "metadata": {},
   "source": [
    "## Experimento 3: Médico tendencioso a classificar como tumor benigno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1bdba564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocos de erro entrada:  [0.   0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.09 0.1  0.11 0.12 0.13\n",
      " 0.14 0.15 0.16 0.17 0.18 0.19 0.2  0.21 0.22 0.23 0.24 0.25 0.26 0.27] Tamanho:  28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28/28 [03:11<00:00,  6.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "percentualErrosMaximo=0.275\n",
    "stepErros=0.01\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "print(\"Blocos de erro entrada: \", vector, \"Tamanho: \", len(vector))\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators = 15,random_state = SEED), # n_estimators =100\n",
    "    \"SVM (RBF)\": SVC(C=100,kernel='rbf', gamma=0.001,class_weight='balanced',random_state = SEED),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators = 20, objective = 'binary:logistic', tree_method = 'hist', eta = 0.1, # n_estimators = 100\n",
    "                             max_depth = 3, enable_categorical = True,random_state = SEED),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='sgd',\n",
    "                    max_iter=500, alpha=0.001,random_state = SEED)\n",
    "}\n",
    "# MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='lbfgs',\n",
    "                    #max_iter=3000, alpha=0.001)\n",
    "\n",
    "nRepeticoes=18\n",
    "resultadosConsolidados_treino = []\n",
    "resultadosConsolidados_teste = []\n",
    "blocos = []\n",
    "all_results=[]\n",
    "resultadosAcc=[]\n",
    "\n",
    "\n",
    "for nivelRuidoAtual in tqdm(vector):\n",
    "    for indiceRepeticao in range(nRepeticoes):\n",
    "        # Total number of samples\n",
    "\n",
    "        dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] # maligno\n",
    "        dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] # benigno\n",
    "\n",
    "        nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "        tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento0 = int(nAmostras_treinamento0 * (nivelRuidoAtual) )\n",
    "\n",
    "        # Randomly select indices for group 1 - treinamento\n",
    "        indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "        indices_label0_erro =np.random.choice(indices_label0_treinamento, size=tamanhoErroTreinamento0, replace=False)\n",
    "\n",
    "        # Select remaining indices for group 1 - teste\n",
    "        indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "        # Determine the size of group 2\n",
    "        nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "        tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento1 = 0\n",
    "\n",
    "        # Randomly select indices for group 2 - treinamento\n",
    "        indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "        indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "        indices_label1_erro =np.random.choice(indices_label1_treinamento, size=tamanhoErroTreinamento1, replace=False)\n",
    "\n",
    "        # introduz erro\n",
    "        dadosLabel0=dadosBrutosLabel0\n",
    "        dadosLabel1=dadosBrutosLabel1\n",
    "        dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"] = dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "        dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"] = dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "\n",
    "        trainData = dadosLabel0.loc[indices_label0_treinamento]\n",
    "        buffer_trainData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "        testData = dadosLabel0.loc[indices_label0_teste]\n",
    "        buffer_testData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "        filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "        filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "        #aleatoriza ordem de treinamento\n",
    "        indicesShuffledTranData = np.random.choice(filtered_rows_Train.index, size=1, replace=False)\n",
    "        trainData = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        trainLabels = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        \n",
    "\n",
    "        trainData = filtered_rows_Train.iloc[:, 1:31]\n",
    "        trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "        testData = filtered_rows_Test.iloc[:, 1:31]\n",
    "        testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "\n",
    "        trainLabels = filtered_rows_Train.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "        testLabels = filtered_rows_Test.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "\n",
    "        # Calcula o nivel de ruido real baseado no número de indices flipados divididio pelo numero total de indices \n",
    "        nivelRuidoAtual_real=(len(indices_label0_erro)+len(indices_label1_erro))/len(trainLabels)\n",
    "        \n",
    "        ### Treinamentos\n",
    "        for name, model in classifiers.items():\n",
    "            model.fit(trainData, trainLabels)\n",
    "            resultadoTreinamento=model.predict(trainData)\n",
    "            resultadoTeste=model.predict(testData)\n",
    "            acc_teste = ( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "            resultadosAcc.append([nivelRuidoAtual_real, name, indiceRepeticao,acc_teste])\n",
    "            for true_label, pred_label in zip(resultadoTeste, testLabels):\n",
    "                all_results.append([nivelRuidoAtual_real, name, indiceRepeticao, true_label, pred_label, acc_teste])\n",
    "\n",
    "        \n",
    "      \n",
    "    # print(\"observados_treinamento=\",observados_treinamento)\n",
    "    # print(\"soma=\",sum(trainLabels==observados_treinamento))æ\n",
    "    # print(\"len=\",len(trainLabels))\n",
    "\n",
    "    # print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "print(blocos)\n",
    "\n",
    "df_results3 = pd.DataFrame(all_results, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"True Label\", \"Predicted Label\", \"Accuracy\"])\n",
    "resultadosExp3 = pd.DataFrame(resultadosAcc, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5254a162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      Noise Level     Classifier  IndexRep  Accuracy\n",
      "0        0.000000  Random Forest         0  0.984026\n",
      "1        0.000000      SVM (RBF)         0  0.977636\n",
      "2        0.000000        XGBoost         0  0.984026\n",
      "3        0.000000            MLP         0  0.904153\n",
      "4        0.000000  Random Forest         1  0.980831\n",
      "...           ...            ...       ...       ...\n",
      "2011     0.143577            MLP        16  0.795527\n",
      "2012     0.143577  Random Forest        17  0.923323\n",
      "2013     0.143577      SVM (RBF)        17  0.865815\n",
      "2014     0.143577        XGBoost        17  0.907348\n",
      "2015     0.143577            MLP        17  0.795527\n",
      "\n",
      "[2016 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(resultadosExp3)\n",
    "string = \"../resultados/experimento/resultadosExperimento3.csv\" \n",
    "if not os.path.exists(string):\n",
    "    resultadosExp3.to_csv(string, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
