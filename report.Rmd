---
title: 'Estudo de Caso 3 - Comparação de desempenho de duas configurações de um algoritmo de otimização'
author: "Ana Júlia de Lima Martins, Antônio Carlos da Anunciação, Melchior Augusto Syrio de Melo"
date: "24 de dezembro de 2024"
output:
  pdf_document:
    fig_caption: yes
  html_document:
    df_print: paged
---

```{r setup,results='hide',warning=FALSE,echo=FALSE}
# A few initial definitions just to make sure all required packages are installed. Change as needed.
# NOTE: It may echo some weird messages to the PDF on the first compile (package installation messages). Run twice and the problem will (hopefully) go away.
suppressPackageStartupMessages({
if (!require(ggplot2, quietly = TRUE)){
      install.packages("ggplot2")
      }
# if (!require(devtools, quietly = TRUE)){
#       install.packages("devtools")
#       }
 if (!require(broom, quietly = TRUE)){
       devtools::install_github("dgrtwo/broom")
      }
# if (!require(GGally, quietly = TRUE)){
#       install.packages("GGally")
#       }
if (!require(dplyr, quietly = TRUE)){
      install.packages("dplyr")
}
if (!require(reshape2, quietly = TRUE)){
      install.packages("reshape2")
}
if (!require(multcomp, quietly = TRUE)){
      install.packages("multcomp")
}})
```

## 1.2. Tamanho amostral

Nesta seção, discutimos a definição do número de instâncias e de repetições necessárias para o experimento, a fim de obter um poder do teste de pelo menos $\pi^* = 0,8$, para detectar diferenças iguais ou superiores a um tamanho de efeito minimamente relevante $d^* = 0,5$, com nível de significância $\alpha = 0,05$.

### 1.2.1. Estimando o número de instâncias (blocos)

Sob a hipótese alternativa H1, a estatística $t_0$ segue uma distribuição t não central (Mathews, 2010) com parâmetro de não centralidade dado por:

$$ncp = \frac{(\mu_D - \mu_0) \sqrt{N}}{\hat{\sigma}_{\Phi}} = \frac{\delta \sqrt{N}}{\hat{\sigma}_\Phi} = d \sqrt{N}.$$
em que $N$ é o número de instâncias necessárias.

Assumindo uma medida de relevância mínima  $d^* = |\delta^*| / \sigma_{\Phi}$, o poder do teste $\pi^*$ é dado pela integral da distribuição t não central com $ncp^* = d^* \sqrt{N}$ sobre os valores de $t_0$ para os quais a hipótese nula $H_0$ é rejeitada (Campelo et. al., 2019):

$$\pi^* = 1 - \beta^* = 1 - \int_{t = t^{(N-1)}_{\alpha/2}}^{t^{(N-1)}_{1-\alpha/2}} \left[ t^{(N-1)}_{\,|\, \text{ncp}^*|} \right] dt$$
Finalmente, o número de instâncias pode ser calculado como o menor inteiro $N$ tal que o poder do teste $\pi^*$ seja igual ou maior que o poder desejado. No caso da hipótese alternativa unilateral, temos que:

$$N^* = \min N \, \bigg| \, t^{(N-1)}_{1-\alpha} \leq t^{(N-1)}_{\beta^* \, ; \, |\text{ncp}^*|}$$


```{r blocos,message=FALSE}
d = 0.5        # Mínima diferença de importância prática (padronizada)
alpha <- 0.05  # Nível de significância
p <- 1 - alpha # Nível de confiança

n <- 2
power <- 0
while (power < 0.8)
{
  df <- n - 1         # n - 1 graus de liberdade
  ncp <- d * sqrt(n)  # Non-centrality parameter

  power <- pt(qt(p,df), df, ncp=ncp, lower.tail=FALSE)
  n <- n + 1
}
n
power
```


### 1.2.2. Estimando o número de repetições por bloco

A abordagem proposta para calcular o número de repetições para o RCBD é similar ao procedimento para um CRD (_Completely Randomized Design_).

Para o CRD, o parâmetro de não-centralidade é definido como:

$$\delta^2 = \frac{n \sum_{i=1}^a (\mu_i - \mu)^2}{\sigma^2}$$
A variabilidade intra-grupo, necessária para calcular o _ncp_, corresponde a variância residual estimada, i.e., a variabilidade não explicada pelo fator experimental e pelo fator bloqueado. Neste experimento, essa variabilidade foi estimada a partir de um estudo piloto com 30 repetições por bloco.

Uma vez definido o _ncp_, a estatística F segue a seguinte distribuição:

$$F \sim\begin{cases}  F_{a-1, a(n-1)} & \text{sob } H_0 \\ F_{a-1, a(n-1), \delta^2} & \text{sob } H_1 \end{cases}$$

O valor crítico $F^*$ para rejeitar $H_0$ com nível de significância $\alpha = 0,05$ é dado por:

$$F^* = F_{a-1, a(n-1), \alpha}$$

Esse valor pode ser calculado usando o seguinte comando em R:

```r
F.crit <- qf(alpha, a-1, a*(n-1), lower.tail = FALSE) # syntax
```

Por definição, o poder do teste é a probabilidade de rejeitar a hipótese nula ($H_0$) quando a hipótese alternativa $H_1$ é verdadeira:

$$\text{Poder} = P(\text{Rejeitar } H_0 | H_1 \text{ é verdadeira}).$$

Isso é equivalente a:

$$P(F(a-1, a(n-1), \delta^2) \geq F^*)$$
Sendo assim, o poder do teste pode ser calculado usando o seguinte comando:

```r
power <- pf(F.crit, a-1, a*(n-1), ncp = ncp, lower.tail = FALSE) # syntax
```

A estratégia consiste em variar o número de repetições $n$, considerando $a = 2$ níveis do fator de interesse, até obter um poder do teste igual ou superior ao poder desejado ($\pi \geq 0,8$)


```{r carregamentoDadosPiloto,message=FALSE}
# Carregar bibliotecas necessárias
library(dplyr)
library(readr)
library(stringr)

# Definir o diretório onde os arquivos estão localizados
diretorio <- "resultados/"  # Substitua pelo caminho correto

# Listar todos os arquivos CSV no diretório
arquivos <- list.files(path = diretorio, pattern = "*teste.csv", full.names = TRUE)

# Função para extrair o nome do algoritmo do nome do arquivo
extrair_algoritmo <- function(nome_arquivo) {
  # Remove o caminho e mantém apenas o nome do arquivo
  nome_limpo <- basename(nome_arquivo)
  # Extrai o algoritmo baseado no padrão do nome do arquivo
  algoritmo <- str_extract(nome_limpo, "MLP|RF|SVM|XGB")
  return(algoritmo)
}

# Ler e combinar os arquivos em um único dataframe, adicionando a coluna "Algoritmo"
dados <- lapply(arquivos, function(arquivo) {
  df <- read.csv(arquivo)
  df$Algoritmo <- extrair_algoritmo(arquivo)  # Adiciona a coluna Algoritmo
  df <- df[, c("Algoritmo", setdiff(names(df), "Algoritmo"))]  # Reorganiza as colunas
  return(df)
}) %>%
  bind_rows()

# Verificar os dados carregados
str(dados)
head(dados)

```


```{r repeticoes2,message=FALSE}
alpha <- 0.05 # nível de significância
d <- 0.5
a <- 2 # Niveis do fator
n_max <- 100 # Número máximo de epetições por instância

# Dataframe para armazenar os resultados
repeticoes <- data.frame(
  Dimensao = integer(),
  Repeticoes = integer(),
  Poder = numeric()
)

for (dim in dimensoes) {
  # Calcula n para cada dimensão para atingir poder >= 0.8
  n <- 2
  power <- 0
  sd <- estatisticas_piloto_df$DesvioPadrao[estatisticas_piloto_df$Dimensao == dim]

  while (power < 0.8)
  {
    df1 <- a - 1 # Graus de liberdade do numerador
    df2 <- a * (n - 1) # Graus de liberdade do denominador
    
    # Calcula as médias observadas para a dimensão atual
    mu_config1 <- mean(estudo_piloto$Fbest[estudo_piloto$Dimensao == dim &
                                           estudo_piloto$Configuracao == "Config1"])
    mu_config2 <- mean(estudo_piloto$Fbest[estudo_piloto$Dimensao == dim &
                                           estudo_piloto$Configuracao == "Config2"])
    # Média global
    mu <- mean(c(mu_config1, mu_config2))

    # Calcula o NCP
    ncp <- (n*((mu_config1 - mu)^2 + (mu_config2 - mu)^2)) / (sd^2)

    F.crit <- qf(alpha, df1, df2, lower.tail = FALSE)

    power <- pf(F.crit, df1, df2, ncp, lower.tail = FALSE)
    
    if (n >= n_max) {
      break
    } else {
      n = n + 1
    }
  }

  # Armazena os resultados no dataframe
  repeticoes <- rbind(repeticoes, data.frame(Dimensao = dim, Repeticoes = n, Poder = power))
  # Salvar resultados em um arquivo CSV
  write.csv(repeticoes, "repeticoes.csv", row.names = FALSE)

}
```


## 3. Análise Exploratória

Antes de realizar os testes de hipótese, foi realizada uma análise exploratória para obter uma visão geral dos dados. A Figura @ref(fig:dataplot) fornece um gráfico para comparar o desempenho médio das duas configuração por dimensão.


```{r dataplot,fig.width=8,echo=TRUE,message=FALSE,label="dataplot",fig.cap="Desempenho médio dos algoritmos por nível de ruído"}
# Carregar bibliotecas
library(ggplot2)
library(dplyr)

# Transformar Percentual de Ruído em fator (se ainda não estiver)
dados$percentualRuidoTreinamento <- as.factor(dados$percentualRuidoTreinamento)

# Agregar os dados: média da acurácia por algoritmo e nível de ruído
aggdata <- aggregate(x = dados$acuracia, 
                     by = list(Algoritmo = dados$Algoritmo, 
                               percentualRuidoTreinamento = dados$percentualRuidoTreinamento), 
                     FUN = mean)

# Rename columns
names(aggdata) <- c("Algoritmo", 
                    "Percentual_Ruido_Treinamento",
                    "Acuracia_Media")

# Coerce categorical variables to factors
for (i in 1:2){
      aggdata[, i] <- as.factor(aggdata[, i])
}

#pdf("./figs/ggplot.pdf", width = 5, height = 5) # <-- uncomment to save plot

# Gráfico de linhas para visualizar o impacto do ruído na acurácia
p <- ggplot(aggdata, aes(x = Percentual_Ruido_Treinamento, 
                         y = Acuracia_Media, 
                         group = Algoritmo, 
                         colour = Algoritmo))
p <- p + geom_line(linetype=2) + geom_point(size=5)
p + labs(title = "Desempenho dos Algoritmos por Nível de Ruído",
          x = "Percentual de Ruído no Treinamento",
          y = "Acurácia Média")

#dev.off() # <-- uncomment this if you uncommented the pdf() command above

```


## 4. Análise Estatística

Para validar nossas observações iniciais, realizamos um teste RCBD para avaliar as diferenças no desempenho das duas configurações.

```{r fitmodel,results='hold'}
model <- aov(Y~Configuracao+Dimensao, data=aggdata)
summary(model)
summary.lm(model)$r.squared
```

A Figura @ref(modelplot) mostra o modelo resultante do teste estatístico.

```{r modelplot,results='hold',label="modelplot"}
par(mfrow = c(2, 2))
plot(model, pch = 20, las = 1)
```

<!-- Como resultado do teste, obteve-se uma estatística F = 34,45 e um p-valor < 2e-16. -->

<!-- Diante do resultado do teste ANOVA, podemos rejeitar, com nível de significância $\alpha = 0,05$, a hipótese nula de que as médias de retorno das cinco ações são iguais. -->

<!-- Esse resultado indica que nem todas as ações têm retornos médios semelhantes e, para identificar quais ações possuem maior retorno médio, realizamos um teste de Tukey. Os resultados do teste de Turkey são mostrados na Figura @ref{fig:turkey}. -->


<!-- ```{r turkey,results='hold',fig.width=8,echo=TRUE,message=FALSE,fig.cap="Comparações múltiplas - Turkey"} -->
<!-- # Comparações múltiplas -->
<!-- library(multcomp) -->

<!-- mc1    <- glht(model, -->
<!--                linfct = mcp(Acoes = "Tukey")) -->
<!-- summary(mc1) -->
<!-- mc1_CI <- confint(mc1, level = 0.95) -->

<!-- par(mar = c(5, 10, 4, 2)) # margens -->
<!-- plot(mc1_CI, -->
<!--      xlab       = "Retornos", -->
<!--      cex.axis   = 1.2, -->
<!--      cex        = 2, -->
<!--      main = "Comparações Múltiplas - Tukey") -->
<!-- ``` -->

<!-- As comparações mais significativas são entre a Ação 2 e as Ações 1, 3, 4 e 5, todas com p-valor < 0,001, indicando que essas ações têm médias de retorno significativamente diferentes. As Ações 1, 3, 4 e 5, por sua vez, apresentam pelo menos uma diferença não significativa quando comparadas entre si. -->

Confirmando a suspeita inicial, o teste de Tukey indica que a Ação 2 tem o maior retorno médio entre as ações analisadas, visto que as comparações entre a Ação 2 e as outras ações todas mostram diferenças significativas.

<!-- ### 5. Verificação das Premissas do Modelo -->
<!-- The assumptions of your test should also be validated, and possible effects of violations should also be explored. -->
<!-- ```{r shapiro,fig.width=8,echo=TRUE,message=FALSE, fig.cap = "Teste de Shapiro-Wilk"} -->
<!-- # Check normality -->
<!-- shapiro.test(retornos_melt) -->

<!-- library(car) -->
<!-- # png(filename = "../figs/paperqq.png", -->
<!-- #     width = 600, height = 600,  -->
<!-- #     bg = "transparent") -->

<!-- qqPlot(retornos_melt,  -->
<!--        pch = 16,  -->
<!--        lwd = 3,  -->
<!--        cex = 2,  -->
<!--        las = 1) -->
<!-- # dev.off() -->

<!-- ``` -->

<!-- # Check homoscedasticity -->
<!-- fligner.test(Retorno ~ Acoes, -->
<!--              data = retornos_melt) -->

<!-- # png(filename = "../figs/papervar.png", -->
<!-- #     width = 600, height = 600,  -->
<!-- #     bg = "transparent") -->
<!-- plot(x    = my.model$fitted.values, -->
<!--      y    = my.model$residuals, -->
<!--      cex  = 2, -->
<!--      las  = 1, -->
<!--      pch  = 16, -->
<!--      xlab = "Fitted values", -->
<!--      ylab = "Residuals") -->
<!-- grid(nx = NULL, ny = NULL,  -->
<!--      lwd = 2, col = "#44444422") -->
<!-- # dev.off() -->
<!-- ``` -->

<!-- ### 6. Conclusões e Recomendações -->
<!-- The discussion of your results, and the scientific/technical meaning of the effects detected, should be placed here. Always be sure to tie your results back to the original question of interest! -->

<!-- ### 7. Atividades Específicas -->

<!-- - Ana Julia: Design do experimento, análise estatística, análise exploratória e descrição do conjunto de dados (redação e código)  -->
<!-- - Antônio:  -->
<!-- - Melchior: -->

### 8. Referências

[1] Mathews, P.: Sample Size Calculations: Practical Methods for Engineers and Scientists, 1st edn. Matthews Malnar & Bailey Inc., Fairport Harbor (2010)

[2] Campelo, F., Takahashi, F. Sample size estimation for power and accuracy in the experimental comparison of algorithms. J Heuristics 25, 305–338 (2019). https://doi.org/10.1007/s10732-018-9396-7