{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "613098c2-149c-4895-ae41-74d2c3ca4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv \n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "percentualTreinamento = 0.7\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "# Define the numeric labels to filter\n",
    "labels={'M','B'}\n",
    "labels_list = list(labels)\n",
    "\n",
    "\n",
    "# Define column names based on dataset documentation\n",
    "columns = [\n",
    "    \"ID\", \"Diagnosis\",\n",
    "    \"Radius_mean\", \"Texture_mean\", \"Perimeter_mean\", \"Area_mean\", \"Smoothness_mean\",\n",
    "    \"Compactness_mean\", \"Concavity_mean\", \"Concave_points_mean\", \"Symmetry_mean\", \"Fractal_dimension_mean\",\n",
    "    \"Radius_se\", \"Texture_se\", \"Perimeter_se\", \"Area_se\", \"Smoothness_se\",\n",
    "    \"Compactness_se\", \"Concavity_se\", \"Concave_points_se\", \"Symmetry_se\", \"Fractal_dimension_se\",\n",
    "    \"Radius_worst\", \"Texture_worst\", \"Perimeter_worst\", \"Area_worst\", \"Smoothness_worst\",\n",
    "    \"Compactness_worst\", \"Concavity_worst\", \"Concave_points_worst\", \"Symmetry_worst\", \"Fractal_dimension_worst\"\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "dadosBrutos = pd.read_csv(\"../../dataset/wdbc.data\", names=columns, header=None)\n",
    "\n",
    "# Drop the ID column (not useful for analysis)\n",
    "dadosBrutos.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "\n",
    "# Filter rows where a column equals one of the labels\n",
    "dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "# metadata \n",
    "#print(dadosBrutos.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd0f445-ffdc-4e84-9290-1a2cc72331eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total number of samples\n",
    "\n",
    "nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 1 - treinamento\n",
    "indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "# Select remaining indices for group 1 - teste\n",
    "indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "# Determine the size of group 2\n",
    "nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 2 - treinamento\n",
    "indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "\n",
    "trainData = dadosBrutosLabel0.loc[indices_label0_treinamento]\n",
    "buffer_trainData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "testData = dadosBrutosLabel0.loc[indices_label0_teste]\n",
    "buffer_testData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "trainData = filtered_rows_Train.iloc[:, 1:31].to_numpy()\n",
    "trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "testData = filtered_rows_Test.iloc[:, 1:31].to_numpy()\n",
    "testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "# aleatoriza ordem de treinamento\n",
    "#indicesShuffledTranData = np.random.choice(trainData.index, size=1, replace=False)\n",
    "#trainData = trainData.loc[indicesShuffledTranData]\n",
    "#trainLabels = trainLabels.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "trainLabels = trainLabels.replace({labels_list[0]: 0, labels_list[1]: 1}).to_numpy()\n",
    "testLabels = testLabels.replace({labels_list[0]: 0, labels_list[1]: 1}) .to_numpy()\n",
    "\n",
    "print(trainLabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cba18-cc5b-4843-92d1-76bffdb47a46",
   "metadata": {},
   "source": [
    "## Experimento 1: Ruído distribuído de forma igual aos dois rótulos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4de44d6-01fc-469d-96f0-99343c4d1925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocos de erro entrada:  [0.    0.006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:08<00:00,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "percentualErrosMaximo=0.05\n",
    "stepErros=0.006\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "print(\"Blocos de erro entrada: \", vector)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators = 15,random_state = SEED), # n_estimators =100\n",
    "    \"SVM (RBF)\": SVC(C=100,kernel='rbf', gamma=0.001,class_weight='balanced',random_state = SEED),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators = 20, objective = 'binary:logistic', tree_method = 'hist', eta = 0.1, # n_estimators = 100\n",
    "                             max_depth = 3, enable_categorical = True,random_state = SEED),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='sgd',\n",
    "                    max_iter=500, alpha=0.001,random_state = SEED)\n",
    "}\n",
    "# MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='lbfgs',\n",
    "                    #max_iter=3000, alpha=0.001)\n",
    "\n",
    "nRepeticoes=10\n",
    "resultadosConsolidados_treino = []\n",
    "resultadosConsolidados_teste = []\n",
    "blocos = []\n",
    "all_results=[]\n",
    "resultadosAcc=[]\n",
    "\n",
    "\n",
    "for nivelRuidoAtual in tqdm(vector):\n",
    "    for indiceRepeticao in range(nRepeticoes):\n",
    "        # Total number of samples\n",
    "\n",
    "        dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "        dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "        nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "        tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento0 = int(nAmostras_treinamento0 * (nivelRuidoAtual/2) )\n",
    "\n",
    "        # Randomly select indices for group 1 - treinamento\n",
    "        indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "        indices_label0_erro =np.random.choice(indices_label0_treinamento, size=tamanhoErroTreinamento0, replace=False)\n",
    "\n",
    "        # Select remaining indices for group 1 - teste\n",
    "        indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "        # Determine the size of group 2\n",
    "        nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "        tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento1 = int(nAmostras_treinamento0 * (nivelRuidoAtual/2) )\n",
    "\n",
    "        # Randomly select indices for group 2 - treinamento\n",
    "        indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "        indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "        indices_label1_erro =np.random.choice(indices_label1_treinamento, size=tamanhoErroTreinamento1, replace=False)\n",
    "\n",
    "        # introduz erro\n",
    "        dadosLabel0=dadosBrutosLabel0\n",
    "        dadosLabel1=dadosBrutosLabel1\n",
    "        dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"] = dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "        dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"] = dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "\n",
    "        trainData = dadosLabel0.loc[indices_label0_treinamento]\n",
    "        buffer_trainData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "        testData = dadosLabel0.loc[indices_label0_teste]\n",
    "        buffer_testData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "        filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "        filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "        #aleatoriza ordem de treinamento\n",
    "        indicesShuffledTranData = np.random.choice(filtered_rows_Train.index, size=1, replace=False)\n",
    "        trainData = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        trainLabels = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "        trainData = filtered_rows_Train.iloc[:, 1:31]\n",
    "        trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "        testData = filtered_rows_Test.iloc[:, 1:31]\n",
    "        testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "\n",
    "        trainLabels = filtered_rows_Train.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "        testLabels = filtered_rows_Test.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "          \n",
    "        # Calcula o nivel de ruido real baseado no número de indices flipados divididio pelo numero total de indices \n",
    "        nivelRuidoAtual_real=(len(indices_label0_erro)+len(indices_label1_erro))/len(trainLabels)\n",
    "        \n",
    "        ### Treinamentos\n",
    "        for name, model in classifiers.items():\n",
    "            model.fit(trainData, trainLabels)\n",
    "            resultadoTreinamento=model.predict(trainData)\n",
    "            resultadoTeste=model.predict(testData)\n",
    "            acc_teste = ( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "            resultadosAcc.append([nivelRuidoAtual_real, name, indiceRepeticao,acc_teste])\n",
    "            for true_label, pred_label in zip(resultadoTeste, testLabels):\n",
    "                all_results.append([nivelRuidoAtual_real, name, indiceRepeticao, true_label, pred_label, acc_teste])\n",
    "\n",
    "        \n",
    "      \n",
    "    # print(\"observados_treinamento=\",observados_treinamento)\n",
    "    # print(\"soma=\",sum(trainLabels==observados_treinamento))æ\n",
    "    # print(\"len=\",len(trainLabels))\n",
    "\n",
    "    # print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "print(blocos)\n",
    "\n",
    "df_results = pd.DataFrame(all_results, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"True Label\", \"Predicted Label\", \"Accuracy\"])\n",
    "resultadosExp = pd.DataFrame(resultadosAcc, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0fae4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Noise Level     Classifier  IndexRep  Accuracy\n",
      "0           0.0  Random Forest         0  0.984026\n",
      "1           0.0      SVM (RBF)         0  0.977636\n",
      "2           0.0        XGBoost         0  0.984026\n",
      "3           0.0            MLP         0  0.904153\n",
      "4           0.0  Random Forest         1  0.980831\n",
      "..          ...            ...       ...       ...\n",
      "75          0.0            MLP         8  0.648562\n",
      "76          0.0  Random Forest         9  0.980831\n",
      "77          0.0      SVM (RBF)         9  0.974441\n",
      "78          0.0        XGBoost         9  0.990415\n",
      "79          0.0            MLP         9  0.961661\n",
      "\n",
      "[80 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(resultadosExp)\n",
    "string = \"../resultados/resultadosExperimento1.csv\" \n",
    "if not os.path.exists(string):\n",
    "    resultadosExp.to_csv(string, index=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1510a78d",
   "metadata": {},
   "source": [
    "## Experimento 2: Médico tendencioso a classificar como tumor maligno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab336fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocos de erro entrada:  [0.    0.006 0.012 0.018 0.024 0.03  0.036 0.042 0.048]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:00<00:03,  2.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:00<00:02,  2.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:01<00:04,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:02<00:03,  1.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 0 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:02<00:02,  1.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:03<00:01,  1.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:03<00:01,  1.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:04<00:00,  1.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:04<00:00,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "percentualErrosMaximo=0.05\n",
    "stepErros=0.006\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "print(\"Blocos de erro entrada: \", vector)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators = 15,random_state = SEED), # n_estimators =100\n",
    "    \"SVM (RBF)\": SVC(C=100,kernel='rbf', gamma=0.001,class_weight='balanced',random_state = SEED),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators = 20, objective = 'binary:logistic', tree_method = 'hist', eta = 0.1, # n_estimators = 100\n",
    "                             max_depth = 3, enable_categorical = True,random_state = SEED),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='sgd',\n",
    "                    max_iter=500, alpha=0.001,random_state = SEED)\n",
    "}\n",
    "# MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='lbfgs',\n",
    "                    #max_iter=3000, alpha=0.001)\n",
    "\n",
    "nRepeticoes=10\n",
    "resultadosConsolidados_treino = []\n",
    "resultadosConsolidados_teste = []\n",
    "blocos = []\n",
    "all_results=[]\n",
    "resultadosAcc=[]\n",
    "\n",
    "\n",
    "for nivelRuidoAtual in tqdm(vector):\n",
    "    for indiceRepeticao in range(nRepeticoes):\n",
    "        # Total number of samples\n",
    "\n",
    "        dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] # maligno\n",
    "        dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "        nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "        tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento0 = 0 # tendencioso a classificar maligno\n",
    "\n",
    "        # Randomly select indices for group 1 - treinamento\n",
    "        indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "        indices_label0_erro =np.random.choice(indices_label0_treinamento, size=tamanhoErroTreinamento0, replace=False)\n",
    "\n",
    "        # Select remaining indices for group 1 - teste\n",
    "        indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "        # Determine the size of group 2\n",
    "        nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "        tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento1 = int(nAmostras_treinamento0 * (nivelRuidoAtual) ) # tendencioso a classificar maligno\n",
    "\n",
    "        # Randomly select indices for group 2 - treinamento\n",
    "        indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "        indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "        indices_label1_erro =np.random.choice(indices_label1_treinamento, size=tamanhoErroTreinamento1, replace=False)\n",
    "\n",
    "        # introduz erro\n",
    "        dadosLabel0=dadosBrutosLabel0\n",
    "        dadosLabel1=dadosBrutosLabel1\n",
    "        dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"] = dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "        dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"] = dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "\n",
    "        trainData = dadosLabel0.loc[indices_label0_treinamento]\n",
    "        buffer_trainData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "        testData = dadosLabel0.loc[indices_label0_teste]\n",
    "        buffer_testData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "        filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "        filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "        #aleatoriza ordem de treinamento\n",
    "        indicesShuffledTranData = np.random.choice(filtered_rows_Train.index, size=1, replace=False)\n",
    "        trainData = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        trainLabels = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "        trainData = filtered_rows_Train.iloc[:, 1:31]\n",
    "        trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "        testData = filtered_rows_Test.iloc[:, 1:31]\n",
    "        testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "\n",
    "        trainLabels = filtered_rows_Train.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "        testLabels = filtered_rows_Test.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "\n",
    "\n",
    "        # Calcula o nivel de ruido real baseado no número de indices flipados divididio pelo numero total de indices \n",
    "        nivelRuidoAtual_real=(len(indices_label0_erro)+len(indices_label1_erro))/len(trainLabels)\n",
    "        \n",
    "        ### Treinamentos\n",
    "        for name, model in classifiers.items():\n",
    "            model.fit(trainData, trainLabels)\n",
    "            resultadoTreinamento=model.predict(trainData)\n",
    "            resultadoTeste=model.predict(testData)\n",
    "            acc_teste = ( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "            resultadosAcc.append([nivelRuidoAtual_real, name, indiceRepeticao,acc_teste])\n",
    "            for true_label, pred_label in zip(resultadoTeste, testLabels):\n",
    "                all_results.append([nivelRuidoAtual_real, name, indiceRepeticao, true_label, pred_label, acc_teste])\n",
    "\n",
    "        \n",
    "      \n",
    "    # print(\"observados_treinamento=\",observados_treinamento)\n",
    "    # print(\"soma=\",sum(trainLabels==observados_treinamento))æ\n",
    "    # print(\"len=\",len(trainLabels))\n",
    "\n",
    "    # print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "print(blocos)\n",
    "\n",
    "df_results2 = pd.DataFrame(all_results, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"True Label\", \"Predicted Label\", \"Accuracy\"])\n",
    "resultadosExp2 = pd.DataFrame(resultadosAcc, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2d2304d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Noise Level     Classifier  IndexRep  Accuracy\n",
      "0      0.000000  Random Forest         0  0.984026\n",
      "1      0.000000      SVM (RBF)         0  0.977636\n",
      "2      0.000000        XGBoost         0  0.984026\n",
      "3      0.000000            MLP         0  0.904153\n",
      "4      0.002519  Random Forest         0  0.987220\n",
      "5      0.002519      SVM (RBF)         0  0.968051\n",
      "6      0.002519        XGBoost         0  0.980831\n",
      "7      0.002519            MLP         0  0.904153\n",
      "8      0.005038  Random Forest         0  0.993610\n",
      "9      0.005038      SVM (RBF)         0  0.987220\n",
      "10     0.005038        XGBoost         0  0.987220\n",
      "11     0.005038            MLP         0  0.945687\n",
      "12     0.007557  Random Forest         0  0.961661\n",
      "13     0.007557      SVM (RBF)         0  0.984026\n",
      "14     0.007557        XGBoost         0  0.961661\n",
      "15     0.007557            MLP         0  0.900958\n",
      "16     0.012594  Random Forest         0  0.977636\n",
      "17     0.012594      SVM (RBF)         0  0.987220\n",
      "18     0.012594        XGBoost         0  0.980831\n",
      "19     0.012594            MLP         0  0.753994\n",
      "20     0.015113  Random Forest         0  0.996805\n",
      "21     0.015113      SVM (RBF)         0  0.977636\n",
      "22     0.015113        XGBoost         0  0.977636\n",
      "23     0.015113            MLP         0  0.904153\n",
      "24     0.017632  Random Forest         0  0.987220\n",
      "25     0.017632      SVM (RBF)         0  0.984026\n",
      "26     0.017632        XGBoost         0  0.968051\n",
      "27     0.017632            MLP         0  0.910543\n",
      "28     0.020151  Random Forest         0  0.980831\n",
      "29     0.020151      SVM (RBF)         0  0.971246\n",
      "30     0.020151        XGBoost         0  0.952077\n",
      "31     0.020151            MLP         0  0.789137\n",
      "32     0.025189  Random Forest         0  0.984026\n",
      "33     0.025189      SVM (RBF)         0  0.993610\n",
      "34     0.025189        XGBoost         0  0.955272\n",
      "35     0.025189            MLP         0  0.907348\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(resultadosExp2)\n",
    "string = \"../resultados/resultadosExperimento2.csv\" \n",
    "if not os.path.exists(string):\n",
    "    resultadosExp2.to_csv(string, index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b622a7",
   "metadata": {},
   "source": [
    "## Experimento 3: Médico tendencioso a classificar como tumor benigno"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1bdba564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocos de erro entrada:  [0.    0.006 0.012 0.018 0.024 0.03  0.036 0.042 0.048]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:00<00:03,  2.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:01<00:03,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:01<00:03,  1.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:01<00:02,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:02<00:02,  1.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0\n",
      " 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:03<00:01,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 1 0 0 0 1 0 0 0 0 0 0 1\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:03<00:01,  1.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0\n",
      " 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:04<00:00,  1.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0\n",
      " 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:04<00:00,  1.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "percentualErrosMaximo=0.05\n",
    "stepErros=0.006\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "print(\"Blocos de erro entrada: \", vector)\n",
    "\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators = 15,random_state = SEED), # n_estimators =100\n",
    "    \"SVM (RBF)\": SVC(C=100,kernel='rbf', gamma=0.001,class_weight='balanced',random_state = SEED),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators = 20, objective = 'binary:logistic', tree_method = 'hist', eta = 0.1, # n_estimators = 100\n",
    "                             max_depth = 3, enable_categorical = True,random_state = SEED),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='sgd',\n",
    "                    max_iter=500, alpha=0.001,random_state = SEED)\n",
    "}\n",
    "# MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='lbfgs',\n",
    "                    #max_iter=3000, alpha=0.001)\n",
    "\n",
    "nRepeticoes=10\n",
    "resultadosConsolidados_treino = []\n",
    "resultadosConsolidados_teste = []\n",
    "blocos = []\n",
    "all_results=[]\n",
    "resultadosAcc=[]\n",
    "\n",
    "\n",
    "for nivelRuidoAtual in tqdm(vector):\n",
    "    for indiceRepeticao in range(nRepeticoes):\n",
    "        # Total number of samples\n",
    "\n",
    "        dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] # maligno\n",
    "        dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] # benigno\n",
    "\n",
    "        nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "        tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento0 = int(nAmostras_treinamento0 * (nivelRuidoAtual) )\n",
    "\n",
    "        # Randomly select indices for group 1 - treinamento\n",
    "        indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "        indices_label0_erro =np.random.choice(indices_label0_treinamento, size=tamanhoErroTreinamento0, replace=False)\n",
    "\n",
    "        # Select remaining indices for group 1 - teste\n",
    "        indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "        # Determine the size of group 2\n",
    "        nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "        tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento1 = 0\n",
    "\n",
    "        # Randomly select indices for group 2 - treinamento\n",
    "        indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "        indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "        indices_label1_erro =np.random.choice(indices_label1_treinamento, size=tamanhoErroTreinamento1, replace=False)\n",
    "\n",
    "        # introduz erro\n",
    "        dadosLabel0=dadosBrutosLabel0\n",
    "        dadosLabel1=dadosBrutosLabel1\n",
    "        dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"] = dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "        dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"] = dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "\n",
    "        trainData = dadosLabel0.loc[indices_label0_treinamento]\n",
    "        buffer_trainData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "        testData = dadosLabel0.loc[indices_label0_teste]\n",
    "        buffer_testData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "        filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "        filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "        #aleatoriza ordem de treinamento\n",
    "        indicesShuffledTranData = np.random.choice(filtered_rows_Train.index, size=1, replace=False)\n",
    "        trainData = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        trainLabels = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        \n",
    "\n",
    "        trainData = filtered_rows_Train.iloc[:, 1:31]\n",
    "        trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "        testData = filtered_rows_Test.iloc[:, 1:31]\n",
    "        testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "\n",
    "        trainLabels = filtered_rows_Train.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "        testLabels = filtered_rows_Test.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "\n",
    "        # Calcula o nivel de ruido real baseado no número de indices flipados divididio pelo numero total de indices \n",
    "        nivelRuidoAtual_real=(len(indices_label0_erro)+len(indices_label1_erro))/len(trainLabels)\n",
    "        \n",
    "        ### Treinamentos\n",
    "        for name, model in classifiers.items():\n",
    "            model.fit(trainData, trainLabels)\n",
    "            resultadoTreinamento=model.predict(trainData)\n",
    "            resultadoTeste=model.predict(testData)\n",
    "            acc_teste = ( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "            resultadosAcc.append([nivelRuidoAtual_real, name, indiceRepeticao,acc_teste])\n",
    "            for true_label, pred_label in zip(resultadoTeste, testLabels):\n",
    "                all_results.append([nivelRuidoAtual_real, name, indiceRepeticao, true_label, pred_label, acc_teste])\n",
    "\n",
    "        \n",
    "      \n",
    "    # print(\"observados_treinamento=\",observados_treinamento)\n",
    "    # print(\"soma=\",sum(trainLabels==observados_treinamento))æ\n",
    "    # print(\"len=\",len(trainLabels))\n",
    "\n",
    "    # print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "print(blocos)\n",
    "\n",
    "df_results3 = pd.DataFrame(all_results, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"True Label\", \"Predicted Label\", \"Accuracy\"])\n",
    "resultadosExp3 = pd.DataFrame(resultadosAcc, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5254a162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Noise Level     Classifier  IndexRep  Accuracy\n",
      "0      0.000000  Random Forest         0  0.984026\n",
      "1      0.000000      SVM (RBF)         0  0.977636\n",
      "2      0.000000        XGBoost         0  0.984026\n",
      "3      0.000000            MLP         0  0.904153\n",
      "4      0.002519  Random Forest         0  0.987220\n",
      "5      0.002519      SVM (RBF)         0  0.968051\n",
      "6      0.002519        XGBoost         0  0.974441\n",
      "7      0.002519            MLP         0  0.843450\n",
      "8      0.005038  Random Forest         0  0.987220\n",
      "9      0.005038      SVM (RBF)         0  0.984026\n",
      "10     0.005038        XGBoost         0  0.987220\n",
      "11     0.005038            MLP         0  0.958466\n",
      "12     0.007557  Random Forest         0  0.974441\n",
      "13     0.007557      SVM (RBF)         0  0.984026\n",
      "14     0.007557        XGBoost         0  0.974441\n",
      "15     0.007557            MLP         0  0.952077\n",
      "16     0.012594  Random Forest         0  0.987220\n",
      "17     0.012594      SVM (RBF)         0  0.984026\n",
      "18     0.012594        XGBoost         0  0.990415\n",
      "19     0.012594            MLP         0  0.942492\n",
      "20     0.015113  Random Forest         0  0.984026\n",
      "21     0.015113      SVM (RBF)         0  0.980831\n",
      "22     0.015113        XGBoost         0  0.984026\n",
      "23     0.015113            MLP         0  0.872204\n",
      "24     0.017632  Random Forest         0  0.990415\n",
      "25     0.017632      SVM (RBF)         0  0.968051\n",
      "26     0.017632        XGBoost         0  0.993610\n",
      "27     0.017632            MLP         0  0.945687\n",
      "28     0.020151  Random Forest         0  0.977636\n",
      "29     0.020151      SVM (RBF)         0  0.971246\n",
      "30     0.020151        XGBoost         0  0.968051\n",
      "31     0.020151            MLP         0  0.932907\n",
      "32     0.025189  Random Forest         0  0.984026\n",
      "33     0.025189      SVM (RBF)         0  0.980831\n",
      "34     0.025189        XGBoost         0  0.980831\n",
      "35     0.025189            MLP         0  0.929712\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(resultadosExp3)\n",
    "string = \"../resultados/resultadosExperimento3.csv\" \n",
    "if not os.path.exists(string):\n",
    "    resultadosExp3.to_csv(string, index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
