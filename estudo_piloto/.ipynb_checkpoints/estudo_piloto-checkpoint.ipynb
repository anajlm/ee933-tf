{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "613098c2-149c-4895-ae41-74d2c3ca4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv \n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "percentualTreinamento = 0.7\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "# Define the numeric labels to filter\n",
    "labels={'M','B'}\n",
    "labels_list = list(labels)\n",
    "\n",
    "\n",
    "# Define column names based on dataset documentation\n",
    "columns = [\n",
    "    \"ID\", \"Diagnosis\",\n",
    "    \"Radius_mean\", \"Texture_mean\", \"Perimeter_mean\", \"Area_mean\", \"Smoothness_mean\",\n",
    "    \"Compactness_mean\", \"Concavity_mean\", \"Concave_points_mean\", \"Symmetry_mean\", \"Fractal_dimension_mean\",\n",
    "    \"Radius_se\", \"Texture_se\", \"Perimeter_se\", \"Area_se\", \"Smoothness_se\",\n",
    "    \"Compactness_se\", \"Concavity_se\", \"Concave_points_se\", \"Symmetry_se\", \"Fractal_dimension_se\",\n",
    "    \"Radius_worst\", \"Texture_worst\", \"Perimeter_worst\", \"Area_worst\", \"Smoothness_worst\",\n",
    "    \"Compactness_worst\", \"Concavity_worst\", \"Concave_points_worst\", \"Symmetry_worst\", \"Fractal_dimension_worst\"\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "dadosBrutos = pd.read_csv(\"../dataset/wdbc.data\", names=columns, header=None)\n",
    "\n",
    "# Drop the ID column (not useful for analysis)\n",
    "dadosBrutos.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "\n",
    "# Filter rows where a column equals one of the labels\n",
    "dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "# metadata \n",
    "#print(dadosBrutos.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ebd0f445-ffdc-4e84-9290-1a2cc72331eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Total number of samples\n",
    "\n",
    "nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 1 - treinamento\n",
    "indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "# Select remaining indices for group 1 - teste\n",
    "indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "# Determine the size of group 2\n",
    "nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 2 - treinamento\n",
    "indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "\n",
    "trainData = dadosBrutosLabel0.loc[indices_label0_treinamento]\n",
    "buffer_trainData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "testData = dadosBrutosLabel0.loc[indices_label0_teste]\n",
    "buffer_testData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "trainData = filtered_rows_Train.iloc[:, 1:31].to_numpy()\n",
    "trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "testData = filtered_rows_Test.iloc[:, 1:31].to_numpy()\n",
    "testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "# aleatoriza ordem de treinamento\n",
    "#indicesShuffledTranData = np.random.choice(trainData.index, size=1, replace=False)\n",
    "#trainData = trainData.loc[indicesShuffledTranData]\n",
    "#trainLabels = trainLabels.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "trainLabels = trainLabels.replace({labels_list[0]: 0, labels_list[1]: 1}).to_numpy()\n",
    "testLabels = testLabels.replace({labels_list[0]: 0, labels_list[1]: 1}) .to_numpy()\n",
    "\n",
    "print(trainLabels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cba18-cc5b-4843-92d1-76bffdb47a46",
   "metadata": {},
   "source": [
    "## Loop Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e4de44d6-01fc-469d-96f0-99343c4d1925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocos de erro entrada:  [0.    0.005 0.01  0.015 0.02  0.025 0.03  0.035 0.04  0.045 0.05  0.055\n",
      " 0.06  0.065 0.07  0.075 0.08  0.085 0.09  0.095]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 20/20 [07:56<00:00, 23.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "percentualErrosMaximo=0.3\n",
    "stepErros=0.005\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "print(\"Blocos de erro entrada: \", vector)\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators = 15), # n_estimators =100\n",
    "    \"SVM (RBF)\": SVC(C=100,kernel='rbf', gamma=0.001,class_weight='balanced'),\n",
    "    \"XGBoost\": XGBClassifier(n_estimators = 20, objective = 'binary:logistic', tree_method = 'hist', eta = 0.1, # n_estimators = 100\n",
    "                             max_depth = 3, enable_categorical = True),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='sgd', max_iter=500, alpha=0.001, random_state = 42)\n",
    "    # \"Random Forest\": RandomForestClassifier(n_estimators=50, max_depth=5, class_weight=\"balanced\"),\n",
    "    # \"SVM (RBF)\": SVC(C=10, kernel='rbf', gamma=0.01, class_weight='balanced', probability=True),\n",
    "    # \"XGBoost\": xgb.XGBClassifier(n_estimators=50, max_depth=4, learning_rate=0.1, tree_method='hist', use_label_encoder=False, eval_metric='logloss'),\n",
    "    # \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=500, alpha=0.001)\n",
    "}\n",
    "# MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='lbfgs',\n",
    "                    #max_iter=3000, alpha=0.001)\n",
    "\n",
    "nRepeticoes=10\n",
    "resultadosConsolidados_treino = []\n",
    "resultadosConsolidados_teste = []\n",
    "blocos = []\n",
    "all_results=[]\n",
    "resultadosAcc=[]\n",
    "\n",
    "\n",
    "for nivelRuidoAtual in tqdm(vector):\n",
    "    for indiceRepeticao in range(nRepeticoes):\n",
    "        # Total number of samples\n",
    "\n",
    "        dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "        dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "        nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "        tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento0 = int(nAmostras_treinamento0 * (nivelRuidoAtual/2) )\n",
    "\n",
    "        # Randomly select indices for group 1 - treinamento\n",
    "        indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "        indices_label0_erro =np.random.choice(indices_label0_treinamento, size=tamanhoErroTreinamento0, replace=False)\n",
    "\n",
    "        # Select remaining indices for group 1 - teste\n",
    "        indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "        # Determine the size of group 2\n",
    "        nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "        tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento1 = int(nAmostras_treinamento0 * (nivelRuidoAtual/2) )\n",
    "\n",
    "        # Randomly select indices for group 2 - treinamento\n",
    "        indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "        indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "        indices_label1_erro =np.random.choice(indices_label1_treinamento, size=tamanhoErroTreinamento1, replace=False)\n",
    "\n",
    "        # Introduz erro\n",
    "        dadosLabel0=dadosBrutosLabel0\n",
    "        dadosLabel1=dadosBrutosLabel1\n",
    "\n",
    "        # Introduzindo erro com base no nível de ruído normal\n",
    "        dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"] = dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "        dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"] = dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "\n",
    "        trainData = dadosLabel0.loc[indices_label0_treinamento]\n",
    "        buffer_trainData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "        testData = dadosLabel0.loc[indices_label0_teste]\n",
    "        buffer_testData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "        filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "        filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "        #aleatoriza ordem de treinamento\n",
    "        indicesShuffledTranData = np.random.choice(filtered_rows_Train.index, size=1, replace=False)\n",
    "        trainData = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        trainLabels = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "        trainData = filtered_rows_Train.iloc[:, 1:31]\n",
    "        trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "        testData = filtered_rows_Test.iloc[:, 1:31]\n",
    "        testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "\n",
    "        trainLabels = filtered_rows_Train.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "        testLabels = filtered_rows_Test.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "          \n",
    "        # Calcula o nivel de ruido real baseado no número de indices flipados divididio pelo numero total de indices \n",
    "        nivelRuidoAtual_real=round((len(indices_label0_erro)+len(indices_label1_erro))/len(trainLabels), 4)\n",
    "        \n",
    "        ### Treinamentos\n",
    "        for name, model in classifiers.items():\n",
    "            model.fit(trainData, trainLabels)\n",
    "            resultadoTreinamento=model.predict(trainData)\n",
    "            resultadoTeste=model.predict(testData)\n",
    "            acc_teste = ( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "            resultadosAcc.append([nivelRuidoAtual_real, name, indiceRepeticao,acc_teste])\n",
    "            for true_label, pred_label in zip(resultadoTeste, testLabels):\n",
    "                all_results.append([nivelRuidoAtual_real, name, indiceRepeticao, true_label, pred_label, acc_teste])\n",
    "\n",
    "\n",
    "print(blocos)\n",
    "\n",
    "df_results = pd.DataFrame(all_results, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"True Label\", \"Predicted Label\", \"Accuracy\"])\n",
    "resultadosExp = pd.DataFrame(resultadosAcc, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "0fae4f63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Noise Level     Classifier  IndexRep  Accuracy\n",
      "0         0.0000  Random Forest         0  0.996094\n",
      "1         0.0000      SVM (RBF)         0  0.968750\n",
      "2         0.0000        XGBoost         0  0.988281\n",
      "3         0.0000            MLP         0  0.863281\n",
      "4         0.0000  Random Forest         1  0.992188\n",
      "..           ...            ...       ...       ...\n",
      "795       0.0806            MLP         8  0.808594\n",
      "796       0.0806  Random Forest         9  0.980469\n",
      "797       0.0806      SVM (RBF)         9  0.937500\n",
      "798       0.0806        XGBoost         9  0.929688\n",
      "799       0.0806            MLP         9  0.796875\n",
      "\n",
      "[800 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(resultadosExp)\n",
    "string = \"./resultados/estudo_piloto.csv\" \n",
    "if not os.path.exists(string):\n",
    "    resultadosExp.to_csv(string, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2748ac-83cb-45c6-8c00-9db95b1f3c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
