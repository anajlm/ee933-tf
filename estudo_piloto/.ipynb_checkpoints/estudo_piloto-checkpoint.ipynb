{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "613098c2-149c-4895-ae41-74d2c3ca4360",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv \n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "percentualTreinamento = 0.7\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "# Define the numeric labels to filter\n",
    "labels={'M','B'}\n",
    "labels_list = list(labels)\n",
    "\n",
    "\n",
    "# Define column names based on dataset documentation\n",
    "columns = [\n",
    "    \"ID\", \"Diagnosis\",\n",
    "    \"Radius_mean\", \"Texture_mean\", \"Perimeter_mean\", \"Area_mean\", \"Smoothness_mean\",\n",
    "    \"Compactness_mean\", \"Concavity_mean\", \"Concave_points_mean\", \"Symmetry_mean\", \"Fractal_dimension_mean\",\n",
    "    \"Radius_se\", \"Texture_se\", \"Perimeter_se\", \"Area_se\", \"Smoothness_se\",\n",
    "    \"Compactness_se\", \"Concavity_se\", \"Concave_points_se\", \"Symmetry_se\", \"Fractal_dimension_se\",\n",
    "    \"Radius_worst\", \"Texture_worst\", \"Perimeter_worst\", \"Area_worst\", \"Smoothness_worst\",\n",
    "    \"Compactness_worst\", \"Concavity_worst\", \"Concave_points_worst\", \"Symmetry_worst\", \"Fractal_dimension_worst\"\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "dadosBrutos = pd.read_csv(\"../dataset/wdbc.data\", names=columns, header=None)\n",
    "\n",
    "# Drop the ID column (not useful for analysis)\n",
    "dadosBrutos.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "\n",
    "# Filter rows where a column equals one of the labels\n",
    "dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "# metadata \n",
    "#print(dadosBrutos.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ebd0f445-ffdc-4e84-9290-1a2cc72331eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# Total number of samples\n",
    "\n",
    "nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 1 - treinamento\n",
    "indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "# Select remaining indices for group 1 - teste\n",
    "indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "# Determine the size of group 2\n",
    "nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 2 - treinamento\n",
    "indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "\n",
    "trainData = dadosBrutosLabel0.loc[indices_label0_treinamento]\n",
    "buffer_trainData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "testData = dadosBrutosLabel0.loc[indices_label0_teste]\n",
    "buffer_testData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "trainData = filtered_rows_Train.iloc[:, 1:31].to_numpy()\n",
    "trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "testData = filtered_rows_Test.iloc[:, 1:31].to_numpy()\n",
    "testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "# aleatoriza ordem de treinamento\n",
    "#indicesShuffledTranData = np.random.choice(trainData.index, size=1, replace=False)\n",
    "#trainData = trainData.loc[indicesShuffledTranData]\n",
    "#trainLabels = trainLabels.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "trainLabels = trainLabels.replace({labels_list[0]: 0, labels_list[1]: 1}).to_numpy()\n",
    "testLabels = testLabels.replace({labels_list[0]: 0, labels_list[1]: 1}) .to_numpy()\n",
    "\n",
    "print(trainLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe6851be-d77e-4d07-85bb-a41419d314f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para simular erro com médicos\n",
    "def add_noise_to_labels(y_train, noise_level, noise_type):\n",
    "    noisy_labels = y_train.copy()\n",
    "    \n",
    "    if noise_type == 'positive':\n",
    "        # Médico 1: sempre positivo (rótulo 1)\n",
    "        noise_indices = np.random.rand(len(y_train)) < noise_level\n",
    "        noisy_labels[noise_indices] = 1  # Atribui 1 para todos os índices escolhidos\n",
    "    elif noise_type == 'negative':\n",
    "        # Médico 2: sempre negativo (rótulo 0)\n",
    "        noise_indices = np.random.rand(len(y_train)) < noise_level\n",
    "        noisy_labels[noise_indices] = 0  # Atribui 0 para todos os índices escolhidos\n",
    "    \n",
    "    return noisy_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cba18-cc5b-4843-92d1-76bffdb47a46",
   "metadata": {},
   "source": [
    "## Loop Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e4de44d6-01fc-469d-96f0-99343c4d1925",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3086842045.py, line 78)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[16], line 78\u001b[0;36m\u001b[0m\n\u001b[0;31m    else if nivelRuidoAtual == 2.0:\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "percentualErrosMaximo=0.05\n",
    "stepErros=0.006\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "vector = np.append(vector, [1.0])  # 1.0 para representar o cenário de médico extremo (sempre positivo)\n",
    "vector = np.append(vector, [2.0])  # 2.0 para representar o cenário de médico extremo (sempre negativo)\n",
    "print(\"Blocos de erro entrada: \", vector)\n",
    "\n",
    "\n",
    "classifiers = {\n",
    "    # \"Random Forest\": RandomForestClassifier(n_estimators = 15), # n_estimators =100\n",
    "    # \"SVM (RBF)\": SVC(C=100,kernel='rbf', gamma=0.001,class_weight='balanced'),\n",
    "    # \"XGBoost\": XGBClassifier(n_estimators = 20, objective = 'binary:logistic', tree_method = 'hist', eta = 0.1, # n_estimators = 100\n",
    "    #                          max_depth = 3, enable_categorical = True),\n",
    "    # \"MLP\": MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='sgd',\n",
    "    #                 max_iter=500, alpha=0.001)\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=50, max_depth=5, class_weight=\"balanced\"),\n",
    "    \"SVM (RBF)\": SVC(C=10, kernel='rbf', gamma=0.01, class_weight='balanced', probability=True),\n",
    "    \"XGBoost\": xgb.XGBClassifier(n_estimators=50, max_depth=4, learning_rate=0.1, tree_method='hist', use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"MLP\": MLPClassifier(hidden_layer_sizes=(100,), activation='relu', solver='adam', max_iter=500, alpha=0.001)\n",
    "}\n",
    "# MLPClassifier(hidden_layer_sizes=(50), activation='tanh', solver='lbfgs',\n",
    "                    #max_iter=3000, alpha=0.001)\n",
    "\n",
    "nRepeticoes=10\n",
    "resultadosConsolidados_treino = []\n",
    "resultadosConsolidados_teste = []\n",
    "blocos = []\n",
    "all_results=[]\n",
    "resultadosAcc=[]\n",
    "\n",
    "\n",
    "for nivelRuidoAtual in tqdm(vector):\n",
    "    for indiceRepeticao in range(nRepeticoes):\n",
    "        # Total number of samples\n",
    "\n",
    "        dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "        dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "        nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "        tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento0 = int(nAmostras_treinamento0 * (nivelRuidoAtual/2) )\n",
    "\n",
    "        # Randomly select indices for group 1 - treinamento\n",
    "        indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "        indices_label0_erro =np.random.choice(indices_label0_treinamento, size=tamanhoErroTreinamento0, replace=False)\n",
    "\n",
    "        # Select remaining indices for group 1 - teste\n",
    "        indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "        # Determine the size of group 2\n",
    "        nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "        tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento1 = int(nAmostras_treinamento0 * (nivelRuidoAtual/2) )\n",
    "\n",
    "        # Randomly select indices for group 2 - treinamento\n",
    "        indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "        indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "        indices_label1_erro =np.random.choice(indices_label1_treinamento, size=tamanhoErroTreinamento1, replace=False)\n",
    "\n",
    "        # Introduz erro\n",
    "        dadosLabel0=dadosBrutosLabel0\n",
    "        dadosLabel1=dadosBrutosLabel1\n",
    "\n",
    "        if nivelRuidoAtual == 1.0:\n",
    "            # Médico 1: sempre diagnóstico positivo (1) para o Label0\n",
    "            dadosLabel0[\"Diagnosis\"] = add_noise_to_labels(dadosLabel0[\"Diagnosis\"], nivelRuidoAtual, \"positive\")\n",
    "        elif nivelRuidoAtual == 2.0:\n",
    "            # Médico 2: sempre diagnóstico negativo (0) para o Label1\n",
    "            dadosLabel1[\"Diagnosis\"] = add_noise_to_labels(dadosLabel1[\"Diagnosis\"], nivelRuidoAtual, \"negative\")\n",
    "        else:\n",
    "            # Introduzindo erro com base no nível de ruído normal\n",
    "            dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"] = dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "            dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"] = dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "\n",
    "        trainData = dadosLabel0.loc[indices_label0_treinamento]\n",
    "        buffer_trainData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "        testData = dadosLabel0.loc[indices_label0_teste]\n",
    "        buffer_testData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "        filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "        filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "        #aleatoriza ordem de treinamento\n",
    "        indicesShuffledTranData = np.random.choice(filtered_rows_Train.index, size=1, replace=False)\n",
    "        trainData = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        trainLabels = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "        trainData = filtered_rows_Train.iloc[:, 1:31]\n",
    "        trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "        testData = filtered_rows_Test.iloc[:, 1:31]\n",
    "        testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "\n",
    "        trainLabels = filtered_rows_Train.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "        testLabels = filtered_rows_Test.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "          \n",
    "        # Calcula o nivel de ruido real baseado no número de indices flipados divididio pelo numero total de indices \n",
    "        nivelRuidoAtual_real=(len(indices_label0_erro)+len(indices_label1_erro))/len(trainLabels)\n",
    "        \n",
    "        ### Treinamentos\n",
    "        for name, model in classifiers.items():\n",
    "            model.fit(trainData, trainLabels)\n",
    "            resultadoTreinamento=model.predict(trainData)\n",
    "            resultadoTeste=model.predict(testData)\n",
    "            acc_teste = ( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "            resultadosAcc.append([nivelRuidoAtual_real, name, indiceRepeticao,acc_teste])\n",
    "            for true_label, pred_label in zip(resultadoTeste, testLabels):\n",
    "                all_results.append([nivelRuidoAtual_real, name, indiceRepeticao, true_label, pred_label, acc_teste])\n",
    "\n",
    "        \n",
    "      \n",
    "    # print(\"observados_treinamento=\",observados_treinamento)\n",
    "    # print(\"soma=\",sum(trainLabels==observados_treinamento))æ\n",
    "    # print(\"len=\",len(trainLabels))\n",
    "\n",
    "    # print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "print(blocos)\n",
    "\n",
    "df_results = pd.DataFrame(all_results, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"True Label\", \"Predicted Label\", \"Accuracy\"])\n",
    "resultadosExp = pd.DataFrame(resultadosAcc, columns=[\"Noise Level\", \"Classifier\", \"IndexRep\", \"Accuracy\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fae4f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "print(resultadosExp)\n",
    "string = \"./resultados/estudo_piloto.csv\" \n",
    "if not os.path.exists(string):\n",
    "    resultadosExp.to_csv(string, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2748ac-83cb-45c6-8c00-9db95b1f3c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
