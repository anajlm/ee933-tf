{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "613098c2-149c-4895-ae41-74d2c3ca4360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Diagnosis  Radius_mean  Texture_mean  Perimeter_mean  Area_mean  \\\n",
      "0         M        17.99         10.38          122.80     1001.0   \n",
      "1         M        20.57         17.77          132.90     1326.0   \n",
      "2         M        19.69         21.25          130.00     1203.0   \n",
      "3         M        11.42         20.38           77.58      386.1   \n",
      "4         M        20.29         14.34          135.10     1297.0   \n",
      "\n",
      "   Smoothness_mean  Compactness_mean  Concavity_mean  Concave_points_mean  \\\n",
      "0          0.11840           0.27760          0.3001              0.14710   \n",
      "1          0.08474           0.07864          0.0869              0.07017   \n",
      "2          0.10960           0.15990          0.1974              0.12790   \n",
      "3          0.14250           0.28390          0.2414              0.10520   \n",
      "4          0.10030           0.13280          0.1980              0.10430   \n",
      "\n",
      "   Symmetry_mean  ...  Radius_worst  Texture_worst  Perimeter_worst  \\\n",
      "0         0.2419  ...         25.38          17.33           184.60   \n",
      "1         0.1812  ...         24.99          23.41           158.80   \n",
      "2         0.2069  ...         23.57          25.53           152.50   \n",
      "3         0.2597  ...         14.91          26.50            98.87   \n",
      "4         0.1809  ...         22.54          16.67           152.20   \n",
      "\n",
      "   Area_worst  Smoothness_worst  Compactness_worst  Concavity_worst  \\\n",
      "0      2019.0            0.1622             0.6656           0.7119   \n",
      "1      1956.0            0.1238             0.1866           0.2416   \n",
      "2      1709.0            0.1444             0.4245           0.4504   \n",
      "3       567.7            0.2098             0.8663           0.6869   \n",
      "4      1575.0            0.1374             0.2050           0.4000   \n",
      "\n",
      "   Concave_points_worst  Symmetry_worst  Fractal_dimension_worst  \n",
      "0                0.2654          0.4601                  0.11890  \n",
      "1                0.1860          0.2750                  0.08902  \n",
      "2                0.2430          0.3613                  0.08758  \n",
      "3                0.2575          0.6638                  0.17300  \n",
      "4                0.1625          0.2364                  0.07678  \n",
      "\n",
      "[5 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import csv \n",
    "import pandas as pd\n",
    "\n",
    "percentualTreinamento = 0.7\n",
    "\n",
    "\n",
    "dataset = {}\n",
    "# Define the numeric labels to filter\n",
    "labels={'M','B'}\n",
    "labels_list = list(labels)\n",
    "\n",
    "\n",
    "# Define column names based on dataset documentation\n",
    "columns = [\n",
    "    \"ID\", \"Diagnosis\",\n",
    "    \"Radius_mean\", \"Texture_mean\", \"Perimeter_mean\", \"Area_mean\", \"Smoothness_mean\",\n",
    "    \"Compactness_mean\", \"Concavity_mean\", \"Concave_points_mean\", \"Symmetry_mean\", \"Fractal_dimension_mean\",\n",
    "    \"Radius_se\", \"Texture_se\", \"Perimeter_se\", \"Area_se\", \"Smoothness_se\",\n",
    "    \"Compactness_se\", \"Concavity_se\", \"Concave_points_se\", \"Symmetry_se\", \"Fractal_dimension_se\",\n",
    "    \"Radius_worst\", \"Texture_worst\", \"Perimeter_worst\", \"Area_worst\", \"Smoothness_worst\",\n",
    "    \"Compactness_worst\", \"Concavity_worst\", \"Concave_points_worst\", \"Symmetry_worst\", \"Fractal_dimension_worst\"\n",
    "]\n",
    "\n",
    "# Load the dataset\n",
    "dadosBrutos = pd.read_csv(\"../dataset/wdbc.data\", names=columns, header=None)\n",
    "\n",
    "# Drop the ID column (not useful for analysis)\n",
    "dadosBrutos.drop(columns=[\"ID\"], inplace=True)\n",
    "\n",
    "\n",
    "# Filter rows where a column equals one of the labels\n",
    "dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "# metadata \n",
    "print(dadosBrutos.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ebd0f445-ffdc-4e84-9290-1a2cc72331eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nf/hkgtx45s3f7_s6zv1l__8jw40000gp/T/ipykernel_45151/392804951.py:38: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  trainLabels = trainLabels.replace({labels_list[0]: 0, labels_list[1]: 1}).to_numpy()\n",
      "/var/folders/nf/hkgtx45s3f7_s6zv1l__8jw40000gp/T/ipykernel_45151/392804951.py:39: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  testLabels = testLabels.replace({labels_list[0]: 0, labels_list[1]: 1}) .to_numpy()\n"
     ]
    }
   ],
   "source": [
    "# Total number of samples\n",
    "\n",
    "nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 1 - treinamento\n",
    "indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "# Select remaining indices for group 1 - teste\n",
    "indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "# Determine the size of group 2\n",
    "nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "\n",
    "# Randomly select indices for group 2 - treinamento\n",
    "indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "\n",
    "trainData = dadosBrutosLabel0.loc[indices_label0_treinamento]\n",
    "buffer_trainData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "testData = dadosBrutosLabel0.loc[indices_label0_teste]\n",
    "buffer_testData = dadosBrutosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "trainData = filtered_rows_Train.iloc[:, 1:31].to_numpy()\n",
    "trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "testData = filtered_rows_Test.iloc[:, 1:31].to_numpy()\n",
    "testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "# aleatoriza ordem de treinamento\n",
    "#indicesShuffledTranData = np.random.choice(trainData.index, size=1, replace=False)\n",
    "#trainData = trainData.loc[indicesShuffledTranData]\n",
    "#trainLabels = trainLabels.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "trainLabels = trainLabels.replace({labels_list[0]: 0, labels_list[1]: 1}).to_numpy()\n",
    "testLabels = testLabels.replace({labels_list[0]: 0, labels_list[1]: 1}) .to_numpy()\n",
    "\n",
    "print(trainLabels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac1f8a57-717a-45cd-847c-a057d9a16a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observados_treinamento= [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1]\n",
      "soma= 397\n",
      "len= 397\n",
      "1.0\n",
      "100.0\n"
     ]
    }
   ],
   "source": [
    "# https://scikit-learn.org/stable/modules/neural_networks_supervised.html\n",
    "\n",
    "clf = svm.SVC(C=20, kernel='rbf', gamma=0.01, class_weight='balanced')\n",
    "\n",
    "clf.fit(trainData,trainLabels)\n",
    "\n",
    "observados_treinamento=clf.predict(trainData)\n",
    "print(\"observados_treinamento=\",observados_treinamento)\n",
    "print(\"soma=\",sum(trainLabels==observados_treinamento))\n",
    "print(\"len=\",len(trainLabels))\n",
    "\n",
    "print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "\n",
    "\n",
    "MSE_treinamento = 1-(1/observados_treinamento.shape[0])*(np.power(np.sum(observados_treinamento-trainLabels),2))\n",
    "print(MSE_treinamento*100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783cba18-cc5b-4843-92d1-76bffdb47a46",
   "metadata": {},
   "source": [
    "## Loop Principal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4de44d6-01fc-469d-96f0-99343c4d1925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocos de erro entrada:  [0.    0.006 0.012 0.018 0.024 0.03  0.036 0.042 0.048]\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "percentualErrosMaximo=0.05\n",
    "stepErros=0.006\n",
    "\n",
    "vector = np.arange(0, percentualErrosMaximo, stepErros)\n",
    "print(\"Blocos de erro entrada: \", vector)\n",
    "nRepeticoes=10\n",
    "resultadosConsolidados_treino = []\n",
    "resultadosConsolidados_teste = []\n",
    "resultadosMedias_teste=[]\n",
    "blocos = []\n",
    "\n",
    "for percentErro in vector:\n",
    "    buffer=[]\n",
    "    for i in range(nRepeticoes):\n",
    "        # Total number of samples\n",
    "\n",
    "        dadosBrutosLabel0 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[0]] \n",
    "        dadosBrutosLabel1 = dadosBrutos[dadosBrutos['Diagnosis'] == labels_list[1]] \n",
    "\n",
    "        nAmostras_treinamento0 = len(dadosBrutosLabel0)\n",
    "        tamanhoTreinamento0 = int(nAmostras_treinamento0 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento0 = int(nAmostras_treinamento0 * (percentErro/2))\n",
    "\n",
    "        # Randomly select indices for group 1 - treinamento\n",
    "        indices_label0_treinamento = np.random.choice(dadosBrutosLabel0.index, size=tamanhoTreinamento0, replace=False)\n",
    "        indices_label0_erro =np.random.choice(indices_label0_treinamento, size=tamanhoErroTreinamento0, replace=False)\n",
    "\n",
    "        # Select remaining indices for group 1 - teste\n",
    "        indices_label0_teste = dadosBrutosLabel0.index.difference(indices_label0_treinamento)\n",
    "\n",
    "        # Determine the size of group 2\n",
    "        nAmostras_treinamento1 = len(dadosBrutosLabel1)\n",
    "        tamanhoTreinamento1 = int(nAmostras_treinamento1 * percentualTreinamento)\n",
    "        tamanhoErroTreinamento1 = int(nAmostras_treinamento0 * (percentErro/2))\n",
    "\n",
    "        # Randomly select indices for group 2 - treinamento\n",
    "        indices_label1_treinamento = np.random.choice(dadosBrutosLabel1.index, size=tamanhoTreinamento1, replace=False)\n",
    "        indices_label1_teste = dadosBrutosLabel0.index.difference(indices_label1_treinamento)\n",
    "        indices_label1_erro =np.random.choice(indices_label1_treinamento, size=tamanhoErroTreinamento1, replace=False)\n",
    "\n",
    "        # introduz erro\n",
    "        dadosLabel0=dadosBrutosLabel0\n",
    "        dadosLabel1=dadosBrutosLabel1\n",
    "        dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"] = dadosLabel0.loc[indices_label0_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "        dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"] = dadosLabel1.loc[indices_label1_erro, \"Diagnosis\"].map({\"M\": \"B\", \"B\": \"M\"})\n",
    "\n",
    "        trainData = dadosLabel0.loc[indices_label0_treinamento]\n",
    "        buffer_trainData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "        testData = dadosLabel0.loc[indices_label0_teste]\n",
    "        buffer_testData = dadosLabel1.loc[indices_label1_treinamento]\n",
    "\n",
    "        filtered_rows_Train = pd.concat([trainData,buffer_trainData], axis=0)\n",
    "        filtered_rows_Test = pd.concat([testData,buffer_testData], axis=0)\n",
    "\n",
    "        #aleatoriza ordem de treinamento\n",
    "        indicesShuffledTranData = np.random.choice(filtered_rows_Train.index, size=1, replace=False)\n",
    "        trainData = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "        trainLabels = filtered_rows_Train.loc[indicesShuffledTranData]\n",
    "\n",
    "\n",
    "        trainData = filtered_rows_Train.iloc[:, 1:31]\n",
    "        trainLabels= filtered_rows_Train.iloc[:, 0]\n",
    "        testData = filtered_rows_Test.iloc[:, 1:31]\n",
    "        testLabels= filtered_rows_Test.iloc[:, 0]\n",
    "\n",
    "\n",
    "        trainLabels = filtered_rows_Train.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "        testLabels = filtered_rows_Test.loc[:, \"Diagnosis\"].map({\"M\": 0, \"B\": 1}).to_numpy()\n",
    "              \n",
    "    \n",
    "        #clf = svm.SVC()\n",
    "        clf = svm.SVC(kernel='rbf', gamma=0.01, class_weight='balanced')\n",
    "\n",
    "        clf.fit(trainData,trainLabels)\n",
    "\n",
    "        resultadoTreinamento=clf.predict(trainData)\n",
    "        resultadoTeste=clf.predict(testData)\n",
    "\n",
    "        blocos.append(  (len(indices_label0_erro)+len(indices_label1_erro))/len(trainLabels) )\n",
    "        resultadosConsolidados_treino.append( sum((trainLabels==resultadoTreinamento))/len(trainLabels) )\n",
    "        resultadosConsolidados_teste.append( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "        buffer.append( sum((testLabels==resultadoTeste))/len(testLabels) )\n",
    "    # print(\"observados_treinamento=\",observados_treinamento)\n",
    "    # print(\"soma=\",sum(trainLabels==observados_treinamento))\n",
    "    # print(\"len=\",len(trainLabels))\n",
    "    resultadosMedias_teste.append(statistics.mean(buffer))\n",
    "    # print(sum(trainLabels==observados_treinamento)/len(trainLabels))\n",
    "\n",
    "print(\"Resultado Treinamento: \", 100*statistics.mean(resultadosConsolidados_treino),\"%\")\n",
    "print(\"Resultado Teste: \", 100*statistics.mean(resultadosConsolidados_teste),\"%\")\n",
    "print(\"Blocos de Erro Reais: \", np.unique(blocos))\n",
    "\n",
    "ig = plt.figure(figsize =(10, 7))\n",
    "\n",
    "# Creating plot\n",
    "#plt.boxplot([resultadosConsolidados_treino,resultadosConsolidados_teste])\n",
    "plt.plot(resultadosMedias_teste)\n",
    "# show plot\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a8e18a1f-8ce7-4362-b355-0ece2e9a9ba9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Média Treinamento: 1.0\n",
      "Média Teste: 0.9959331597222222\n",
      "900\n",
      "0.9986979166666666\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"Média Treinamento:\", np.mean(resultadosConsolidados_treino))\n",
    "print(\"Média Teste:\", np.mean(resultadosConsolidados_teste))\n",
    "\n",
    "print(len(resultadosConsolidados_treino))\n",
    "\n",
    "bloco_formatted_values = [f\"{v:.3f}\" for v in np.unique(blocos)]\n",
    "\n",
    "group_column = np.repeat(bloco_formatted_values, nRepeticoes)[:len(resultadosConsolidados_treino)]  # Adjust length\n",
    "\n",
    "dataTreino = pd.DataFrame({\n",
    "    'acuracia': resultadosConsolidados_treino,\n",
    "    'percentualRuidoTreinamento': group_column\n",
    "})\n",
    "string = \"../resultados/SVM_breast_treinamento.csv\" \n",
    "if not os.path.exists(string):\n",
    "    dataTreino.to_csv(string, index=True)\n",
    "\n",
    "dataTeste = pd.DataFrame({\n",
    "    'acuracia': resultadosConsolidados_teste,\n",
    "    'percentualRuidoTreinamento': group_column\n",
    "})\n",
    "string = \"../resultados/SVM_breast_teste.csv\" \n",
    "if not os.path.exists(string):\n",
    "    dataTeste.to_csv(string, index=True)\n",
    "\n",
    "print(statistics.mean(resultadosConsolidados_teste[20:29]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
